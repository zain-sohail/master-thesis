
Rather than calling it denoising, better word in image reconstruction because
Image Reconstruction:

Purpose: To reconstruct an image from incomplete, noisy, or indirect measurements. This is often used in medical imaging (e.g., MRI, CT scans), computational photography, and computer vision applications. 

Reconstruction involves generating a complete image from partial or indirect data, which can include denoising and deblurring as sub-tasks.

\section{Address reconstruction/denoising schemes}
VST with BM3D: BM3D uses collaborative filtering, which is also used in recommender systems [citation need]
UNET noise2noise

Why ARPES transfer learning can't work for our case.


Most methods assume a Gaussian noise model, whether in classical or DL approaches. So we need to see the noise type first.

"Owing to solve the clean image x from the Eq. (1) is an ill-posed problem, we cannot get the unique solution from the image model with noise. To obtain a good estimation image 
, image denoising has been well-studied in the field of image processing over the past several years. Generally, image denoising methods can be roughly classified as [3]: spatial domain methods, transform domain methods, which are introduced in more detail in the next couple of sections." from \href{https://vciba.springeropen.com/articles/10.1186/s42492-019-0016-7}{source}

\section{Metrics}
\gls{PSNR}, \gls{SSIM}, \gls{MSE}, \gls{MAE}, Huber loss, Poisson loss are examples of metrics used to evaluate the quality of the reconstructed image. These metrics measure the similarity between the true image and the reconstructed image, and can be used to compare different reconstruction algorithms.

PSNR: Peak Signal-to-Noise Ratio can be written as:
\begin{equation}
    \text{PSNR} = 10 \log_{10} \left( \frac{255^2}{\text{MSE}} \right)
\end{equation}

SSIM: Structural Similarity Index Measure can be written as:
\begin{equation}
    \text{SSIM} = \frac{(2\mu_x\mu_y + C_1)(2\sigma_{xy} + C_2)}{(\mu_x^2 + \mu_y^2 + C_1)(\sigma_x^2 + \sigma_y^2 + C_2)}
\end{equation}

\section{BM3D: Densoising in sparse domain}
with and without anscombe on different datasets from flash, lab and fhi
Testing s
\subsection{Anscombe: Variance Stabilization Transform}

\section{Statistical Learning}
One can do a lot of different learning approaches. We will build towards the deep learning approach. Networks which can deal with image data are generally Convolutional Neural Networks. Looking at research, UNET has been used for image segmentation and denoising, which combines the concept of autoencoders and convolutional neural networks, along with skip connections.
We use this approach with the noise2noise framework, which is a deep learning framework for image reconstruction.

For classical learning algorithms, the learning problem is not always realizable, meaning that not always is the 
ERM 
Deep learning is just linear sepeartor problem with a non linear function applied to it like RELU

Our aim is to reduce the error between the true image and the reconstructed image. This is a regression problem, where we are trying to learn a function that maps the noisy image to the true image. Mathematically, this can be written as:

ERM: Empirical Risk Minimization
\begin{equation}
    \hat{f} = \argmin_f \frac{1}{n} \sum_{i=1}^{n} (f(x_i) - y_i)^{2}
\end{equation}

Training error and empirical risk 

Popular loss functions include the mean squared error (MSE), mean absolute error (MAE), and Huber loss. The choice of loss function depends on the noise model and the desired properties of the reconstruction. For example, the MSE is commonly used for Gaussian noise, while the MAE is more robust to outliers.

The MSE is defined as:
\begin{equation}
    \text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (f(x_i) - y_i)^{2}
\end{equation}

MSE is the squared L2 norm of the difference between the predicted and true values. It is sensitive to outliers and can be dominated by large errors. The MAE is defined as:

\begin{equation}
    \text{MAE} = \frac{1}{n} \sum_{i=1}^{n} \left| f(x_i) - y_i \right|
\end{equation}

\subsection{Optimal Loss Function}
Optimal loss can be derived from the distribution model. The Method of Moments and Maximum Likelihood Estimation are two common methods for deriving the optimal loss function. 

For Poisson noise, the optimal loss function is the negative log-likelihood of the Poisson distribution. This is because the Poisson distribution is the maximum entropy distribution for count data, and the negative log-likelihood is the maximum likelihood estimator for the Poisson distribution.
This can be written as the following optimization problem:
\begin{equation}
    \hat{f} = \argmin_f -\sum_{i=1}^{n} \log \left( \frac{e^{-f(x_i)} f(x_i)^{y_i}}{y_i!} \right)
\end{equation}


\subsection{Regularization}
There are different ways to regularize the loss function to prevent overfitting. This can be done by adding a penalty term to the loss function. Common regularization techniques include L1 and L2 regularization, which add the absolute value of the weights and the square of the weights to the loss function, respectively. This can be written as:
\begin{equation}
    \hat{f} = \argmin_f \frac{1}{n} \sum_{i=1}^{n} (f(x_i) - y_i)^{2} + \lambda \sum_{j=1}^{p} \left| \beta_j \right|
\end{equation}

\subsection{Optimization}
\section{Noise2Noise: Deep Learning framework}
Important to take care not to train with empty data. 

Here we take care of data generation. Finite Capture Budget. We use the Graphene on Iridium dataset. The Noisy realizations are just less counts binned. 
E.g. 96M counts as Noisy and 186M counts as Target. Or 8M counts as Noisy and 96M counts as Target.
\subsection{Convolutional Neural Networks}
\subsection{Autoencoder}