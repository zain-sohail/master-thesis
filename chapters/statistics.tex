To attempt at reconstructing the truth from a noisy observation, it is essential to understand the underlying noise statistics. Classical and quantum optics provide a comprehensive theoretical foundation to explain photon statistics. For instance, it is well understood that photons from a coherent light source follow a Poisson distribution, often referred to as shot-noise. Whereas, for chaotic (bunched) light, the variance exceeds that compared to their mean, dubbed super-Poissonian. And finally for a squeezed light source, the distribution is sub-Poissonian \cite[Chapter~5]{foxQuantumOpticsIntroduction2006}.

In this chapter, we start with defining the Poisson model for counting photo-electron event data. We examine our data to identify where this model is appropriate and where its limitations become evident. To this end, we introduce some statistical apparatus necessary to estimate parameters and define goodness of model. Further, we explore why the \gls{FEL} light source (see section \ref{section:light-sources}), exhibits over-dispersed statistics, a characteristic necessitating moving beyond the Poisson model. Accounting for this, we transition to using the Negative Binomial distribution as a more suitable model for the count data. Lastly, we acknowledge that theoretically these models primarily address photon statistics and a more sophisticated treatment building on light-matter interaction might be necessary to connect photon statistics with the statistics of the emitted electrons.


\section{Poisson distribution as a Model for Photon Counting Statistics}\label{section:photon-counting-stats}

Photons are the quantized form of electromagnetic light. They can be thought of as discrete energy packets. The energy of a photon is given by $E = h\nu$, where $h$ is the Planck constant and $\nu$ is the frequency of the light. Due to the discrete nature of photons, it can be shown that the number of photons in a short time interval $\Delta t$ is not constant. These fluctuations are known as photon shot noise.

For a light source with a constant flux\footnote{Flux is the average number of photons passing through the cross-section of a beam per unit time.} $\phi$ such as a single-mode laser, the average number of photons in a beam segment of length is given by $L$, $\lambda = \phi \frac{L}{c}$, where $c$ is the speed of light.

If we subdivide this $L$ into many small intervals  of size $L/N$, where $N$ is large enough so that there is low probability of a photon being in an interval, eventually there will be divisions with no photons, divisions with only single photon, and negligible divisions with multiple photons. For all possible orderings, the probability of finding $n$ subdivisions with a single photon and $(N-n)$ with no photons can be modeled by the Binomial distribution as follows, with $p=\frac{\lambda}{N}$ being the probability of a photon being in a segment:

\begin{equation}
    P(n) = \binom{N}{n} p^n (1 - p)^{N - n}
\end{equation}

Using the Poisson Limit Theorem \cite{fellerIntroductionProbabilityTheory1968}, it can be shown that as $N \to \infty$, and the probability $p \to 0$ such that $Np = \lambda$ remains constant, there is a convergence in distribution to the Poisson distribution.

\begin{note}
    {Poisson distribution}
    The \gls{PMF} for Poisson distribution Poi\((\lambda)\) with \(\lambda > 0\) is defined as
    \begin{equation}
        P(n;\lambda) = \frac{\lambda^n e^{-\lambda}}{n!}, \quad n \in \mathbb{N}_0
    \end{equation}
    \begin{enumerate}
        \item $\lambda = E(X) = Var(X)$ 
        \item Additivity: If $X_1 \sim \text{Poi}(\lambda_1)$ and $X_2 \sim \text{Poi}(\lambda_2)$ are independent Poisson random variables, then the sum $X_1 + X_2$ also follows a Poisson distribution with parameter $\lambda_1 + \lambda_2$
        \begin{equation}
            X_1 + X_2 \sim \text{Poi}(\lambda_1 + \lambda_2).
        \end{equation}
        
        This property is useful when considering counts from multiple independent sources.
    \end{enumerate}
\end{note}

Hence, the Poisson distribution is a suitable model for counting statistics of photons from a coherent light source.

\section{Poisson distribution as a Model for Photoelectron Counting Statistics}\label{section:photoelectron-counting-stats}

Through usage of only classical arguments, \citeauthor{mandelFluctuationsPhotonBeams1958} \cite{mandelFluctuationsPhotonBeams1958,mandelFluctuationsPhotonBeams1959} derived the probability distribution for photoelectron statistics. We assume that every photoelectron counts is a statistically independent event. And for the photoelectron to be ejected in a very short time interval $\Delta t$, the probability should be directly proportional to the intensity of light falling on the detector.

Since the derivation is lengthy, the reader is referred to \cite{mehtaVIIITheoryPhotoelectron1970}, and the results are directly presented.
\begin{note}
    {Probablity distribution of Photoelectron counting}
    \begin{equation}\label{eq:mandel-photo-electron}
        P(n, t, T) = \int_{0}^{\infty} \frac{\alpha W^n}{n!} e^{-\alpha W} P(W) \, dW
    \end{equation}
    with $\alpha$ being the quantum efficiency, with $W$ being the integrated light intensity ($I$) over the time interval $\Delta t$:
    \begin{equation}
        W = \int_{t}^{t+T} I(t') dt'
    \end{equation}
    and $P(W)$ being the probability density of the random variable $W$.

    For a constant intensity light source, the probability distribution simplifies to the Poisson distribution:
    \begin{equation}
        P(n, t, T) = \frac{W^n e^{-W}}{n!} 
    \end{equation}
\end{note}

We can see from \ref{eq:mandel-photo-electron} that even with intensity of light being constant, there are random fluctuations. However, it also implies that if there are fluctuations in the intensity of light, the photoelectron counting statistics will not be Poissonian. We shall later see that this is an important argument to go beyond the Poissonian model for \gls{FEL} light sources.

\section{Correlated events in detector}
Owing to the 2-layered \gls{DLD} in our experimental setup, measuring two electrons impinging on locations nearby to one another can be detected. In a single layer \gls{DLD}, due to the dead-time experienced by each sector after recording, only a single event can be recorded. However, the multi-layer detection scheme comes with a caveat: the layers must be calibrated appropriately otherwise the electron counting routines can report two events for the same electron. 

For reconstructing multi-hit events in the detector, method such as deep learning have been used in \cite{knipferDeepLearningbasedSpatiotemporal2024}


There's a lot of parameters that need to be tested to determine what sort of counting statistics the dataset has.

Controls for the test:
lets see
\begin{itemize}
    \item Total time being looked at (like 1000 s or 20 hours)
    \begin{itemize}
        \item distribution might change due to overtime FEL intensity changes
    \end{itemize}
    \item Time bins being used (like 2 s vs 20 s and so on).
    \begin{itemize}
        \item Seems like distribution changes based on that too
    \end{itemize}
    \item Looking at individual pixels on X and Y
    \item Looking at Energy axis as it behaves weirder
    \item Looking at a larger region in X and Y 
    \begin{itemize}
        \item should follow same statisitics as single pixels
    \end{itemize}
    \item Check after removing correlated electrons within each pulse
    \begin{itemize}
        \item It is possible that electrons are correlated between different pulses because the time delay is long enough. But seems highly unlikely!
    \end{itemize}
    \begin{itemize}
        \item 
    \end{itemize}
\end{itemize}


\subsection{Before filtering}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{images/image.png}
    \caption{Enter Caption}
    \label{Image at one 2D pixel but summed over energy}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{images/summed.png}
    \caption{Image over 40 pixels and summed over energy}
    % \label{fig:enter-label}
\end{figure}

\subsection{After filtering}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{images/poisson_stats/after_filtering_region_1000s.png}
    \caption{Data is filtered here with the KNN and looking at small region}
    % \label{fig:enter-label}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{images/poisson_stats/filtered_allY_singleX.png}
    \caption{Enter Caption}
    % \label{fig:enter-label}
\end{figure}


Taken from \cite{berteroImageDeblurringPoisson2009} which cites \cite{fellerIntroductionProbabilityTheory1968}
\section{Simulate Noise}
We simulate the data with Poissonian data.
\section{Statistical testing}

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{images/2024-08-27-15.11.15.png}
    \caption{Enter Caption}
\end{figure}




\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{images/JD-54-image.png}
    \caption{Enter Caption}
    \label{fig:enterl}
\end{figure}

% \section{Using CLT}
% Central Limit Theorem (CLT)
The \textbf{Central Limit Theorem} states that for a sequence of i.i.d. random variables \(X_1, X_2, \ldots, X_n\) with mean \( \mu \) and variance \( \sigma^2 \), the normalized sum of these variables approaches a standard normal distribution as \(n\) tends to infinity:

\[
\frac{\sum_{i=1}^{n} X_i - n\mu}{\sigma \sqrt{n}} \xrightarrow{d} N(0, 1) \quad \text{as} \quad n \to \infty.
\]

This convergence is in distribution.
\begin{quotation}
    From the law of large numbers, one can show that the relative fluctuations reduce as the reciprocal square root of the number of throws, a result valid for all statistical fluctuations, including shot noise. From Wikipedia
    
    Shot noise exists because phenomena such as light and electric current consist of the movement of discrete (also called "quantized") 'packets'.
\end{quotation}

Can't correlate FEL intensity with electron counts per pulse as the GMD is before the monochromator. 

\begin{quotation}
    The physical assumptions which we want to express mathematically are that the \textbf{conditions of the experiment remain constant} in time*, and that non-overlapping time intervals are stochastically independent in the sense that information concerning the number of events in one interval reveals nothing about the other. The theory of probabilities in a continuum makes it possible to express these statements directly, but being restricted to discrete probabilities, we have to use an approximate finite model and pass to the limit.

    Imagine a unit time interval partitioned into n subintervals of length 1/n. A given collection of finitely many points in the interval may be regarded as the result of a chance process such that each subinterval has the same probability $P_{n}$ to contain one or more points of the collection. A subinterval is then either occupied or empty, and the assumed independence of non-overlapping time intervals implies that we are dealing with Bernoulli trials: We assume that the probability for exactly k occupied subintervals is given by $b(k;n,P_{n})$. We now refine this discrete model indefinitely by letting n→inf. The probability that the whole interval contains no point of the collection must tend to a finite limit. But this is the event that no cell is occupied, and its probability is $(1-p_{n})^{n}$. Passing to logarithms it is seen that this quantity approaches a limit only if $np_{n}$
    from book \cite{fellerIntroductionProbabilityTheory1968}
\end{quotation}

\section{Chi-squared Goodness of Fit Test}
We hypothesize that the data follows a certain distribution e.g. Poisson, Normal, Negative Binomial The Chi-squared Goodness of Fit Test is used to determine if the observed data is consistent with the expected distribution.

Let \( X_1, X_2, \ldots, X_n \sim \text{i.i.d. } F \) and \( Y_1, Y_2, \ldots, Y_m \sim \text{i.i.d. } G \), where \( F \) and \( G \) are strictly increasing continuous \glsplural{CDF}.

The hypotheses for the chi-square test are:

\begin{equation}
    H_0: F = G \leftrightarrow   H_1: F \neq G
\end{equation}
The hypothesis test for the Poisson distribution is the Chi-squared Goodness of Fit Test. The test statistic is given by:

\begin{equation}
    \chi^2 = \sum_{i=1}^{k} \frac{(O_i - E_i)^2}{E_i}
\end{equation}
where \(O_i\) is the observed frequency and \(E_i\) is the expected frequency. The degrees of freedom are given by \(k-1\), where \(k\) is the number of bins.

For goodness-of-fit tests, small p-values indicate that you can reject the null hypothesis and conclude that your data were not drawn from a population with the specified distribution. Consequently, goodness-of-fit tests are a rare case where you look for high p-values to identify candidate distributions. 
(From \href{https://statisticsbyjim.com/hypothesis-testing/goodness-fit-tests-discrete-distributions/}{This website})


\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{images/violin_plots_per_pulse.png}
    \caption{Enter Caption}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\linewidth]{images/violinplots_per_train.png}
    \caption{ss}
    \label{s}   
\end{figure}
\section{Modeling over-dispersed count data}

\section{The SASE process as an explanation of over-dispersed statistics}

