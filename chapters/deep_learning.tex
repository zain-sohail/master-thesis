In Chapter~\ref{ch:pes-statistics}, we saw that the photoemission statistics are influenced by the characteristics of the light source, that exhibit well-defined statistical properties. However, when using multidimensional detection schemes capturing millions of voxels, the complexity of the process renders the process difficult to model. Furthermore, the predictive power of such model would rely on an existing understanding the materials, the very aspect under investigation. While the physical process is intriguing to study from a physical perspective, it is not practical for the purpose of image restoration.

Instead of using explicit models such as \gls{BM3D} that encode prior knowledge about the data corruption (e.g. \gls{AWGN}, or Poisson noise as discussed in \cref{sec:poisson-noise-model}), it is also possible to learn the underlying model based on the data. This approach of not hard coding knowledge, and learning complex mappings based on data is the foundation of machine and deep learning, allowing to extract meaningful information from incomplete measurements.

Learning can be broadly categorized in two categories: the \textit{supervised} and the \textit{unsupervised} setting. In supervised learning, the experience (\textit{model}) is learned by exposure to data containing information additional (labeled data) to what the model is supposed to map. The exposure to such data, called \textit{training}, enables the model to establish a relationship between the inputs and their corresponding outputs. The model’s expertise can then be used to map new inputs to their corresponding outputs. In the context of image restoration, the model can gain expertise from pairs of corrupted and clean images, and then use this expertise to restore other corrupted images.

Conversely, in the unsupervised setting, the model is exposed to the data without any labeled information, and the model is supposed to learn the underlying structure of the data. In context of image restoration, that would imply only giving the model the corrupted images, and expecting to map to the desired target--the clean image.

This chapter, we will discuss the foundations of learning, covering the elements of how and why learning works. This is general to both the supervised and unsupervised setting. There the concepts of \textit{hypothesis class}, \textit{capacity}, \textit{realizability}, and the concept of \textit{generalization} capability are discussed. Then we take a look at a statistical learning\footnote{Machine learning and statistical learning are many times used interchangeably but the latter places more emphasis on the statistical properties of the learning algorithms.} paradigm known as \textit{\gls{ERM}}, aiming to minimize the training error/empirical risk. \Gls{ERM} comes with its own set of challenges, such as overfitting in hypothesis spaces with high \textit{capacity}, which can be mitigated by \textit{regularization}. 

We discuss the combination of \textit{\gls{CNN}} and \textit{Autoencoders}, commonly used in image restoration tasks. Later, \textit{Noise2Noise}, a learning framework for image restoration not requiring clean images, introduced by \citeauthor{lehtinenNoise2NoiseLearningImage2018}, is discussed. To fully explain this framework, we look at the \textit{loss function} that allows this paradigm to work. We use the concrete realization using the \texttt{UNET} architecture, introduced by \citeauthor{ronnebergerUNetConvolutionalNetworks}, and its 3D variant, \texttt{UNET3D}.

These methods are then applied to train a model that maps incomplete observations from \gls{PES} to the true multidimensional image, with $d=3$ dimensions. The aspects of data generation, training, and evaluation are henceforth, discussed in detail.

Much of the foundational concepts discussed are based on \cite{shalev-shwartzUnderstandingMachineLearning2014a,jamesIntroductionStatisticalLearning2013,tibshiraniElementsStatisticalLearning,goodfellowDeepLearning2016}.

\section{Foundations of Learning}
Statistical regression is a classical example of a learning algorithm, where the goal of regression is to learn a function that maps the input data to the output data.
Let us approach the image restoration problem from this perspective. Using the general observation model defined in \cref{eq:observation-model}, the learning problem is then to find a hypothesis $h$ that maps the degraded image $X$ to the true image $Y$ that generalizes well.

\begin{equation}
    h: X \mapsto Y
\end{equation}

In Machine Learning, we do not make any explicit assumption of the \textit{data generating distribution} except that all instances of the data are \textit{\gls{iid}} and generated according to a distribution $\mathcal{D}$ i.e. $Y, X \sim \mathcal{D}$.

\subsection{Generalization}
\textit{Generalization} refers to the ability of a learning algorithm to perform well on new, unseen data. The goal of learning is to find a hypothesis $h$ that generalizes well to new data. 

If the data generating distribution $\mathcal{D}$ is \gls{iid} and known, the \textit{generalization error}/\textit{population risk} $\mathcal{R}(\theta)$, with model parameters $\theta$, can be minimized to find the optimal hypothesis $\hat{h}^*$. This is often the case in classical regression.

$\mathcal{R}(\theta)$ is defined as expectation taken across the data generating distribution $\mathcal{D}$, with the predicted output of a hypothesis $h(x; \theta)$ and the true output $y$:

\begin{equation}
    \mathcal{R}(\theta) = \mathbb{E}_{(x, y) \sim \mathcal{D}} \left[ \ell(h(x; \theta), y) \right]
\end{equation}

where the loss $\ell$ measures the difference between two quantities, such as of $h(x; \theta)$ and $y$.

Through optimization, the optimal hypothesis $\hat{h}^*$ can be found that minimizes the expected risk $\mathcal{R}$:

\begin{equation}
    \hat{h}^* = \argmin_{h \in \mathcal{H}} \mathcal{R}(\mathcal{D}; \theta)
\end{equation}

We already saw Wiener filtering in \cref{eq:wiener-mse} that minimizes the \gls{MSE} loss, a case of the risk minimization. Since it is a linear filter, the hypothesis space $\mathcal{H}$ is constrained to linear functions.

\subsection{Hypothesis Class, Capacity and Realizability}
The hypothesis $h$ is chosen from a hypothesis space $\mathcal{H}$, where the hypothesis space $\mathcal{H}$ is the set of functions that the learning algorithm can choose from to approximate the true function. 
For example, in linear regression, the hypothesis space $\mathcal{H}$ consists of all possible linear functions of the form:

\begin{equation}\label{eq:linear-hypothesis}
   \mathcal{H} =  \left\{ h_{\mathbf{w}}(\mathbf{x}) = \mathbf{w}^\top \mathbf{x} + w_0 \mid \mathbf{w} \in \mathbb{R}^d, w_0 \in \mathbb{R} \right\}
\end{equation}

The \textit{capacity} (the measure of size\footnote{The VC dimension and Rademacher complexity are two popular measures of capacity.}) of this hypothesis space is smaller than other hypothesis spaces such as generalized linear models
\footnote{These are linear models with a non-linear activation function, such as sigmoid function.} 
or polynomial class of functions. Choosing a hypothesis class with a larger capacity allows for more complex functions to be learned, but we will see that this comes with a trade-off.

We want to find a space that makes our learning problem \textit{realizable}. This means that there exists a mapping in the hypothesis space that can perfectly model the true mapping. A counter example could be if our data generation function is a $sin(x)$ function, but we choose a linear hypothesis space, then our problem is not realizable. In most real-world scenarios, the true space for complex data such as images is seldom known, and we forego the realizability condition. This is the \textit{agnostic} learning setting. For classes with high capacity, even if they can not perfectly model the true function, they might approximate it well.

\subsection{Empirical Risk Minimization}

In the case where true data distribution $\mathcal{D}$ is unknown, and the learning algorithm must make predictions on new data that sampled from $\mathcal{D}$.
The most popular learning algorithm is known as \textit{\glsxtrlong{ERM}}. For a training set $\mathcal{S} = \left\{ (x_1, y_1), \ldots, (x_n, y_n) \right\}$ where $\mathcal{S} \sim \mathcal{D}^n$, and model parameters $\theta$, the empirical risk $\mathcal{L}(\theta; \mathcal{S})$ is defined as:

\begin{equation}
    \mathcal{L}(\mathcal{S}; \theta) = \frac{1}{\lvert \mathcal{S} \rvert} \sum_{(x, y) \in \mathcal{S}} \ell(h(x; \theta), y),
\end{equation}

where $\ell$ is the loss function that measures the error between the predicted output $h(x; \theta)$ and the true output $y$ mapped from $\mathcal{G} (x)$. The \gls{ERM} algorithm finds the hypothesis $\hat{h}$ from hypothesis space $\mathcal{H}$ that minimizes the training error $\mathcal{L}$

\begin{equation}
    \hat{h} = \argmin_{h \in \mathcal{H}} \mathcal{L}(\theta; \mathcal{S})
\end{equation}

We shall later see that the assumption of output  $y = \mathcal{G} (x)$ can be relaxed in the \textit{Noise2Noise} framework with an \gls{MSE} $\ell$.

\subsection{Regularization}
The \gls{ERM} algorithm runs the risk of \textit{overfitting}. This means that the hypothesis $\hat{h}$ might perform well on the training data $\mathcal{S}$ but poorly on new data drawn from the same distribution $\mathcal{D}$. Usually, this happens for rich hypothesis classes (high capacity), or $\mathcal{S}$ is not representative of $\mathcal{D}$ and the \gls{ERM} algorithm chooses a hypothesis that is too complex. 

To counter this, \textit{regularization} techniques are used. A penalty term based on model parameters is added to prevent overfitting. The \gls{ERM} then finds the hypothesis $\hat{h}$ that minimizes the regularized loss function\footnote{This is also known as Regularizated Loss Minimization (RLM)}:

\begin{equation}
    \hat{h} = \argmin_{h \in \mathcal{H}} \left(\mathcal{L}(\theta; \mathcal{S})  + \lambda R(\theta) \right).
\end{equation}

where $R(\theta)$ is the regularization term that penalizes complex models, and $\lambda$ is the parameter controlling the trade-off between the training error and the regularization term. Common regularization techniques include L1 (Lasso) and L2 (Ridge) regularization. L1 regularization can be written as:

\begin{equation*}
    R(\theta) = \|\theta\|_1 = \sum{j} |\theta_j|
\end{equation*}

and L2 regularization as:

\begin{equation*}
    R(\theta) = \|\theta\|^2_2 = \sum_{j} \theta_j^2
\end{equation*}

Regularization can also be seen as a way of restricting the hypothesis space $\mathcal{H}$ by encoding prior knowledge into the model. In the case of image restoration, it might be known a priori that the image is smooth, and  can encode this prior knowledge by adding a penalty term that penalizes sharp changes in the image.


\subsection{Uniform Convergence}
It is always possible that with a small probability, the training data is not representative of the data distribution $\mathcal{D}$. Hence, every learning algorithm has a confidence and accuracy level that, in practice, is hard to quantify. For simpler cases, these bounds can be theoretically proven but practically, other evaluation methods are assumed e.g. by empirically evaluating some test data, we can ascertain if the learned model generalizes well.

For a hypothesis space $\mathcal{H}$ that is finite, and \gls{iid} training data all drawn from the same distribution $\mathcal{D}$, the \gls{ERM} algorithm can be shown to have a generalization error that converges to zero as the number of training samples $n$ goes to infinity. This is known as the \textit{uniform convergence} property of the \gls{ERM} algorithm. This can further be generalized to infinite hypothesis spaces.


The finite hypothesis space assumption makes sense, considering that we learn through discrete data, and infinite hypothesis spaces become finite. 

\section{Neural Networks for Image Restoration}
There are a myriad of learning algorithms based on \gls{ERM}, that aim to minimize the expected loss $\mathcal{L}$ over a training set $\mathcal{S}$.  We have linear regression that aims to minimize the \gls{MSE} loss, logistic regression for binary classification (softmax as a generalization for multi-class classification) that minimizes the cross-entropy loss, and support vector machines that minimize the hinge loss to maximize the margin between different classes.

Let us now look at one broad class of learning algorithms that have shown remarkable success in countless applications, including image restoration: \textit{Neural Networks}.

We do not discuss other learning algorithms

If we generate images with overlapping subsets, the formed images are not independent, as the same events could be present in multiple images. A simple example would be to generate image an image with \num{e6} counts and other with twice as many counts, where the former is a subset of the latter. This is a limitation of the method, but it is a reasonable approximation for the purpose of this study.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{images/simple_nn_architecture.pdf}
    % \includesvg{images/simple_nn_architecture.svg}
    \caption{An example trained fully connected neural network architecture with an input layer, two hidden layers, and an output layer. The weights are represented by the lines connecting the neurons, with opacity indicating the weights' strength.}
    \label{fig:simple-nn-architecture}
\end{figure}
Neural networks (NNs) represent a class of models that draw inspiration from the structural and functional characteristics of the biological brain. These models are designed to approximate complex functions by learning from data. A neural network consists of interconnected nodes, referred to as "neurons," which process input data to generate output. In the realm of supervised learning, neural networks are utilized to map input data—such as images—to desired outputs, for instance, restored or enhanced images.

\subsection{Basic Structure of Neural Networks}

% A typical neural network comprises several layers of neurons. The input layer serves as the first layer, receiving the input data, with each node corresponding to a specific feature of that data. Following the input layer, the network contains one or more hidden layers. These intermediate layers are crucial for computation and learning, as each neuron within a hidden layer computes a weighted sum of its inputs, applies a nonlinear activation function, and transmits the resulting value to the subsequent layer. Finally, the output layer produces the final output of the network based on the computations performed in the hidden layers.

% Connections between neurons are associated with weights that determine the strength and direction of the signals transmitted between them. The learning process of the network is achieved through the adjustment of these weights, which is accomplished via a method known as backpropagation. This process aims to minimize the error between the predicted output and the actual target by modifying the weights according to the gradient of the loss function.

\subsection{Activation Functions}

% To introduce non-linearity into the model, each neuron typically applies an activation function to the weighted sum of its inputs. Common activation functions include the sigmoid function, which maps input values to a range between 0 and 1, making it particularly useful in binary classification tasks. The Rectified Linear Unit (ReLU) sets negative inputs to zero while preserving positive inputs, and it is widely favored due to its simplicity and effectiveness in training deep networks. Additionally, the hyperbolic tangent (tanh) function maps input values to a range between -1 and 1, and it is often employed in the hidden layers of neural networks. The selection of an appropriate activation function significantly influences the network's performance and its ability to learn complex patterns within the data.

\subsection{Training Neural Networks}

% Training a neural network involves two primary phases: the forward pass and the backward pass. During the forward pass, the input data is propagated through the network, layer by layer, to generate an output. Following this, the backward pass occurs, in which the error—defined as the difference between the predicted output and the true output—is propagated back through the network using the gradient of the loss function. This step is crucial for updating the weights of the network.

% The objective of training is to minimize the loss function, which quantifies the discrepancy between the predicted output and the true output. Common loss functions include Mean Squared Error (MSE), which is often used in regression tasks, and Cross-Entropy, which is prevalent in classification tasks. The training process utilizes optimization algorithms, such as Stochastic Gradient Descent (SGD) or Adam, which iteratively adjust the network's weights to reduce the loss.

\subsection{Universal Approximation Theorem}

% A fundamental result in the theoretical framework of neural networks is the Universal Approximation Theorem. This theorem posits that a feedforward neural network, possessing at least one hidden layer and a sufficient number of neurons, can approximate any continuous function on a compact subset of \(\mathbb{R}^n\) to any desired degree of accuracy, provided that the network employs a non-linear activation function.


% The theorem can be formally articulated as follows: For any continuous function \(f: \mathbb{R}^n \to \mathbb{R}\) and for any \(\epsilon > 0\), there exists a feedforward neural network \(N\) with at least one hidden layer, a finite number of neurons, and an activation function (such as sigmoid or ReLU), such that for every \(x \in \mathbb{R}^n\), the inequality 

% $|f(x) - N(x)| < \epsilon$

% holds true. This result underscores the potential of neural networks; with an adequate number of neurons, they are capable of approximating complex, high-dimensional functions, independent of the underlying distribution of the data.

% The Universal Approximation Theorem carries significant implications for the field of machine learning. Primarily, it highlights the expressive power of neural networks, which, in theory, can approximate any function. This versatility renders them highly effective for a wide array of tasks, including image classification, restoration, and speech recognition. 

% While the theorem assures that a shallow network, possessing a single hidden layer, can approximate any function, practical applications often benefit from the utilization of deeper networks, which contain multiple hidden layers. Deeper networks typically require fewer neurons to achieve a comparable level of approximation and possess the capability to learn hierarchical representations of data. This characteristic is particularly advantageous in tasks involving images and language.

% Despite the theoretical capabilities outlined by the Universal Approximation Theorem, practical challenges persist in training these models effectively. Issues such as local minima, vanishing gradients, and overfitting can complicate the training process. Furthermore, the number of neurons necessary to achieve the desired accuracy may be prohibitively large, leading to substantial computational demands.

% In summary, the Universal Approximation Theorem emphasizes the considerable potential of neural networks to model intricate patterns. However, the practical success of a neural network is contingent upon careful design, tuning, and training of the model.

\section{Noise2Noise: Deep Learning framework}
% One can do a lot of different learning approaches. We will build towards the deep learning approach. Networks which can deal with image data are generally Convolutional Neural Networks. Looking at research, UNET has been used for image segmentation and denoising, which combines the concept of autoencoders and convolutional neural networks, along with skip connections.
% We use this approach with the noise2noise framework, which is a deep learning framework for image reconstruction.

% For classical learning algorithms, the learning problem is not always realizable, meaning that not always is the 
% ERM 
% Linear separators are restrictive
% Deep learning is just linear sepeartor problem with a non linear function applied to it like RELU



% Important to take care not to train with empty data. \cite{lehtinenNoise2NoiseLearningImage2018}

% Here we take care of data generation. Finite Capture Budget. We use the Graphene on Iridium dataset. The Noisy realizations are just less counts binned. 
% E.g. 96M counts as Noisy and 186M counts as Target. Or 8M counts as Noisy and 96M counts as Target.
\subsection{Convolutional Neural Networks}
\subsection{Autoencoder}
\cite{goodfellowDeepLearning2016}
\subsection{UNET}



\section{Training}

\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{images/training_flowchart.pdf}
    \caption{Flowchart illustrating the input-target dataset pairs used for neural network training, derived from subsets of the \gls{GrIr} dataset. A total of \num{34} unique noisy subsets are represented, with each dataset corresponding to one blue box in the chart. The different combinations shown in the chart generate \num{51} input-target pairs across various counts, including \numlist{1e6;2e6;4e6;8e6;1.6e7;3.2e7;4.8e7;9.6e7}. Notably, the \num{4.8e7} and \num{9.6e7} counts serve as the target datasets, with \num{9.6e7} additionally functioning as the target for the \num{4.8e7} datasets.}
    \label{fig:training-data}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{images/images_noisy_denoised_with_target.pdf}
    \caption{Noisy and denoised \gls{ky}-\gls{kx} cuts with window size $w=1$ shown for \gls{GrIr} dataset. Each column corresponds to \numlist{1e6;2e6;4e6;8e6;1.6e7;3.2e7;1.86e8} counts, respectively; where the last column is the target image with $w=1$ in row 1 and $w=15$ in row 2. Below \num{8e6} counts, none of the features are discernible for noisy images, while the denoised images show clear features similar to the target.}
    \label{fig:images-noisy-denoised-training}
\end{figure}

\section{Testing}
\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{images/nn_gdw_msssim.pdf}
    \caption{\num{1e6} counts take approximately \qty{11.5}{min} to acquire. \gls{GdW} dataset.}
    \label{fig:gdw-test-metirc}
\end{figure}


\begin{figure}
    \centering
    \begin{subfigure}[b]{1\linewidth}
        \centering
        \includegraphics[width=1\linewidth]{images/nn_denoised_ex_w_1.pdf}
        \caption{}
        \label{fig:nn-denoised-ex-w-1}
    \end{subfigure}

    \begin{subfigure}[b]{1\linewidth}
        \centering
        \includegraphics[width=1\linewidth]{images/nn_denoised_ex_w_20.pdf}
        \caption{\num{4e6} count dataset. The denoising performance leaves room for improvement, using the adjusted optimal $\sigma_{\text{o}}\approx0.4$.}
        \label{fig:nn-denoised-ex-w-20}
    \end{subfigure}
    \caption{}
    \label{fig:nn-denoised-ex-w}
\end{figure}

\begin{figure}
    \centering
    \begin{subfigure}[b]{1\linewidth}
        \centering
        \includegraphics[width=1\linewidth]{images/nn_denoised_xy_w_1.pdf}
        \caption{}
        \label{fig:nn-denoised-xy-w-1}
    \end{subfigure}

    \begin{subfigure}[b]{1\linewidth}
        \centering
        \includegraphics[width=1\linewidth]{images/nn_denoised_xy_w_20.pdf}
        \caption{}
        \label{fig:nn-denoised-xy-w-20}
    \end{subfigure}
    \caption{}
    \label{fig:nn-denoised-xy-w}
\end{figure}