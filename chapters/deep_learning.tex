In Chapter~\ref{ch:pes-statistics}, we saw that the photoemission statistics are influenced by the characteristics of the light source, that exhibit well-defined statistical properties. Understanding these specific characteristics can be helpful in designing tailored, model-based denoisers. For Poisson noise, we employed the Anscombe transform combined with \gls{BM3D} (\cref{sec:poisson-noise-model}). While the Anscombe transform is capable of stabilizing Poisson-distributed noise into approximately Gaussian noise for moderate to high counts, it is less effective for very low counts ($<3$), where the approximation breaks down. Similarly, a \gls{VST} tailored for \gls{NB} statistics in conjunction with \gls{BM3D} could be more effective to reduce noise.

Model-based approaches such as these can be powerful but assume that the noise characteristics are accurately known and that the underlying signal conforms to the priors encoded. However, \gls{FEL}-based photoemission experiments involve complex and heterogeneous noise sources. Factors such as long-term fluctuations in \gls{FEL} intensity (\cref{section:fel-stats}), advanced detector designs (\cref{section:8s-dld}) and environmental variations can lead to deviations from idealized models like Poisson or \gls{NB} noise prior. Adapting model-based methods to account for these complexities requires significant effort and may still fall short when dealing with unknown or dynamically changing noise patterns.

Unlike model-based methods, which depend on predefined assumptions about the noise and other image priors, machine and deep learning methods can learn these relationships directly from data. This data-driven approach allows deep learning models to handle diverse and complex noise distributions, including those that are non-standard, multi-modal, or vary across a dataset. Furthermore, \gls{CNN}-based architectures excel at extracting non-linear and hierarchical features from such high-dimensional data, structures typical of \gls{MPES} datasets. This allows to take advantage of the spatial and temporal correlations across multiple dimensions more effectively than many traditional approaches.

Machine learning can be broadly categorized in two categories: the \textit{supervised} and the \textit{unsupervised} setting. In supervised learning, the experience (\textit{model}) is learned by exposure to data containing information additional (labeled data) to what the model is supposed to map. The exposure to such data, called \textit{training}, enables the model to establish a relationship between the inputs and their corresponding outputs. The modelâ€™s expertise can then be used to map new inputs to their corresponding outputs. In the context of image restoration, the model can gain expertise from pairs of corrupted and clean images, and then use this expertise to restore other corrupted images.

Conversely, in the unsupervised setting, the model is exposed to the data without any labeled information, and the model is supposed to learn the underlying structure of the data. In context of image restoration, that would imply only giving the model the corrupted images, and expecting to map to the desired target--the clean image.

This chapter, we will discuss the foundations of learning, covering the elements of how and why learning works. This is general to both the supervised and unsupervised setting. There the concepts of \textit{hypothesis class}, \textit{capacity}, \textit{realizability}, and the concept of \textit{generalization} capability are discussed. Then we take a look at a statistical learning\footnote{Machine learning and statistical learning are many times used interchangeably but the latter places more emphasis on the statistical properties of the learning algorithms, and the former on the algorithmic aspects.} paradigm known as \textit{\gls{ERM}}, aiming to minimize the training error/empirical risk. \Gls{ERM} comes with its own set of challenges, such as overfitting in hypothesis spaces with high \textit{capacity}, which can be mitigated by \textit{regularization}. 

We discuss the combination of \textit{\gls{CNN}} and \textit{Autoencoders}, commonly used in image restoration tasks. Later, \gls{noise2noise}, a learning framework for image restoration not requiring clean images, introduced by \citeauthor{lehtinenNoise2NoiseLearningImage2018}, is discussed. To fully explain this framework, we look at the \textit{loss function} that allows this paradigm to work. We apply this training scheme using the concrete realization \texttt{UNET3D} architecture, the 2D variant introduced first by \citeauthor{ronnebergerUNetConvolutionalNetworks}.

These methods are then applied to train a model that maps incomplete observations from \gls{PES} to the true multidimensional image, with $d=3$ dimensions. The aspects of data generation, training, and evaluation are henceforth, discussed in detail.

Much of the foundational concepts discussed are based on \cite{shalev-shwartzUnderstandingMachineLearning2014a,jamesIntroductionStatisticalLearning2013,tibshiraniElementsStatisticalLearning,goodfellowDeepLearning2016}.

\section{Foundations of Learning}
Statistical regression is a classical example of a learning algorithm, where the goal of regression is to learn a function that maps the input data to the output data.
Let us approach the image restoration problem from this perspective. Using the general observation model defined in \cref{eq:observation-model}, the learning problem is then to find a hypothesis $h$ that maps the degraded image $X$ to the true image $Y$ that generalizes well.
\begin{equation}
    h: X \mapsto Y
\end{equation}

In machine learning, we do not make any explicit assumption of the \textit{data generating distribution} except that all instances of the data are \textit{\gls{iid}} and generated according to a distribution $\mathcal{D}$ i.e.\ $Y, X \sim \mathcal{D}$. Continuing forward, since the discussion is not restricted to images, we denote input and target as $x$ and $y$ respectively. 

\subsection{Generalization}\label{sec:generalization}
\Gls{generalization} refers to the ability of a learning algorithm to perform well on new, unseen data. The goal of learning is to find a hypothesis $h$ that generalizes well to new data. 

If the data generating distribution $\mathcal{D}$ is \gls{iid} and known, the \textit{generalization error}/\textit{population risk} $\mathcal{R}(\theta)$, with model parameters $\theta$, can be minimized to find the optimal hypothesis $\hat{h}^*$. $\mathcal{R}(\theta)$ is defined as expectation taken across the data generating distribution $\mathcal{D}$, with the predicted output of a hypothesis $h(x; \theta)$ and the true output $y$:
\begin{equation}\label{eq:risk}
    \mathcal{R}(\theta) = \mathbb{E}_{(x, y) \sim \mathcal{D}} \left[ \ell(h(x; \theta), y) \right]
\end{equation}
where the loss $\ell$ measures the difference between two quantities, such as of $h(x; \theta)$ and $y$.

Through optimization, the optimal hypothesis $\hat{h}^*$ can be found that minimizes the expected risk $\mathcal{R}$:
\begin{equation}\label{eq:risk-min}
    \hat{h}^* = \argmin_{h \in \mathcal{H}} \mathcal{R}(\mathcal{D}; \theta)
\end{equation}

\subsection{Hypothesis Class, Capacity and Realizability}
The hypothesis $h$ is chosen from a hypothesis space $\mathcal{H}$, where the hypothesis space $\mathcal{H}$ is the set of functions that the learning algorithm can choose from to approximate the true function. 
For example, in linear regression, the hypothesis space $\mathcal{H}$ consists of all possible linear functions\footnote{We also already an example of a problem where the hypothesis space $\mathcal{H}$ is constrained to linear functions: the linear minimum \gls{MSE} estimator, Wiener filter discussed in \cref{sec:wiener-filter}.} of the form:
\begin{equation}\label{eq:linear-hypothesis}
   \mathcal{H} =  \left\{ h_{\mathbf{w}}(\mathbf{x}) = \mathbf{w}^\top \mathbf{x} + w_0 \mid \mathbf{w} \in \mathbb{R}^d, w_0 \in \mathbb{R} \right\}
\end{equation}
where $\mathbf{w}$ is the weight vector, $\mathbf{x}$ is the input vector, and $w_0$ is the bias term. 

The \textit{capacity} (the measure of size\footnote{The VC dimension and Rademacher complexity are two popular measures of capacity.}) of this hypothesis space is smaller than other hypothesis spaces such as polynomial class of functions. Choosing a hypothesis class with a larger capacity allows for more complex functions to be learned, but we will see that this comes with a trade-off.

We want to find a space that makes our learning problem \textit{realizable}. This means that there exists a mapping in the hypothesis space that can perfectly model the true mapping. An example for an unrealizable problem is if the data generation function is a $\sin(x)$ function, but the learning algorithm has a linear hypothesis space. In most real-world scenarios, the true space for complex data such as images is seldom known, and we forego the realizability condition. This is the \textit{agnostic} learning setting. For classes with high capacity, even if they can not perfectly model the true function, they might approximate it well.

\subsection{Empirical Risk Minimization}\label{sec:erm}
When the true data distribution $\mathcal{D}$ is unknown, the population risk can no longer directly be minimized. Instead, we approximate it by minimizing the empirical risk. For a training set $\mathcal{S} = \left\{ (x_1, y_1), \ldots, (x_n, y_n) \right\}$ where $\mathcal{S} \sim \mathcal{D}^n$, and model parameters $\theta$, the empirical risk $\mathcal{L}(\theta; \mathcal{S})$ (also known as \gls{training_error}) is defined as:
\begin{equation}
    \mathcal{L}(\theta; \mathcal{S}) = \frac{1}{\lvert \mathcal{S} \rvert} \sum_{(x, y) \in \mathcal{S}} \ell(h(x; \theta), y),
\end{equation}
where $\ell$ is the loss function that measures the error between the predicted output $h(x; \theta)$ and the true output $y$. \Glsxtrfull{ERM} finds the hypothesis $\hat{h}$ from hypothesis space $\mathcal{H}$ that minimizes the training error $\mathcal{L}$
\begin{equation}\label{eq:erm}
    \hat{h} = \argmin_{h \in \mathcal{H}} \mathcal{L}(\mathcal{S};\theta)
\end{equation}
by optimizing the model parameters $\theta$.
We shall later see that the assumption of having access to a true $y$ can be relaxed in the \textit{Noise2Noise} framework using $L_2$ (\gls{MSE}) as the loss $\ell$.

The gap between the $\mathcal{L}(\theta; \mathcal{S})$ and $\mathcal{R}(\theta)$ is known as generalization error and a hypothesis with a large gap generally implies \textit{overfitting}.

\subsection{Regularization}
The \gls{ERM} algorithm runs the risk of overfitting. This means that the hypothesis $\hat{h}$ might perform well on the training data $\mathcal{S}$ but poorly on new data drawn from the same distribution $\mathcal{D}$. Usually, this happens for rich hypothesis classes (high capacity), or when $\mathcal{S}$ is not representative of $\mathcal{D}$ and the \gls{ERM} algorithm chooses a hypothesis that is too complex. 

To counter this, \textit{regularization} techniques are used. A penalty term based on model parameters is added to prevent overfitting. The \gls{ERM} then finds the hypothesis $\hat{h}$ that minimizes the regularized loss function\footnote{This is also known as Regularizated Loss Minimization.}:
\begin{equation}
    \hat{h} = \argmin_{h \in \mathcal{H}} \left(\mathcal{L}(\theta; \mathcal{S})  + \lambda R(\theta) \right).
\end{equation}
where $R(\theta)$ is the regularization term that penalizes complex models, and $\lambda$ is the parameter controlling the trade-off between the training error and the regularization term. Common regularization techniques include $L_1$ (Lasso) and $L_2$ (Ridge) regularization. $L_1$ regularization can be written as:
\begin{equation*}
    R(\theta) = \|\theta\|_1 = \sum{j} |\theta_j|
\end{equation*}
and $L_2$ regularization as:
\begin{equation*}
    R(\theta) = \|\theta\|^2_2 = \sum_{j} \theta_j^2
\end{equation*}

Regularization can also be seen as a way of restricting the hypothesis space $\mathcal{H}$ by encoding prior knowledge into the model. In the case of image restoration, it might be known a priori that the image is smooth, and  can encode this prior knowledge by adding a penalty term that penalizes sharp changes in the image.


\subsection{Uniform Convergence}
It is always possible that with a small probability, the training data is not representative of the data distribution $\mathcal{D}$. Hence, every learning algorithm has a confidence and accuracy level that, in practice, is hard to quantify. For simpler cases, these bounds can be theoretically proven but practically, other evaluation methods are assumed, e.g.\ by empirically evaluating some test data, we can ascertain if the learned model generalizes well.

For a hypothesis space $\mathcal{H}$ that is finite, and \gls{iid} training data all drawn from the same distribution $\mathcal{D}$, the \gls{ERM} algorithm can be shown to have a generalization error that converges to zero as the number of training samples $n$ goes to infinity. This is known as the \textit{uniform convergence} property of the \gls{ERM} algorithm. This can further be generalized to infinite hypothesis spaces, through non-uniform convergence. However, considering that most machine learning takes place through discrete data, making infinite hypothesis spaces finite, the finite hypothesis space is a reasonable assumption.
% \todo[inline]{Stopping point for review currently.}
% \dots
\section{Learning Algorithms}
There are a myriad of supervised learning algorithms built upon the foundation of \gls{ERM}, that aim to minimize the expected loss by minimizing the empirical risk (\cref{eq:erm}). Some examples include linear regression that minimizes the \gls{MSE} loss, logistic regression for binary classification\footnote{With softmax regression as a generalization for multi-class classification. The term "regression" is conventionally used, but this is actually a classification task.} that minimizes the cross-entropy loss, and support vector machines, that can be framed as a hinge loss minimization problem, aim to maximize the margin between different classes \cite{bishopPatternRecognitionMachine2006}.

Let us develop towards one broad class of learning algorithms that have shown remarkable success in countless applications \todocite{prob DL denoising citations would help here}, including image restoration: \textit{Neural Networks}.

\subsection{Linear Regression and Perceptrons}
Linear regression\footnote{We briefly saw in \cref{eq:linear-hypothesis} the hypothesis space of linear regression being all linear functions.} is among thesimplest forms of machine learning models, seeking to model a relationship between the continuous output $y \in \mathbb{R}$ as a weighted sum of input features $\mathbf{x} \in \mathbb{R}^d$ with an added bias term $w_0$:
\begin{equation}\label{eq:linear-regression}
    y(\mathbf{x}) = \mathbf{w}^\top \mathbf{x} + w_0
\end{equation}
The model parameters can be learned based on \gls{ERM} i.e.\ minimizing a loss function to learn the weights. For example, using the \gls{MSE} loss, the objective becomes:
\begin{equation}\label{eq:mse-loss}
    \ell(\mathbf{w}, w_0) = \frac{1}{n} \sum_{i=1}^{n} (y_i - \mathbf{w}^\top \mathbf{x}_i - w_0)^2
\end{equation}
where $n$ is the number of training samples, and $(\mathbf{x}_i, y_i)$ are the input-target pairs. This can be minimized in closed-form to find the optimal weights $\hat{\mathbf{w}}$ and bias $\hat{w}_0$ \cite{bishopPatternRecognitionMachine2006}.

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/nn_perceptron.pdf}
        \caption{A computational graph of a standard perceptron. The input features $\mathbf{x}$ are linearly combined with the weights $\mathbf{w}$ and bias $w_0$ to produce the output $y$.}
        \label{fig:nn-perceptron}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/nn_multiclass.pdf}
        \caption{A computational graph of a multi-output perceptron. The input features $\mathbf{x}$ are linearly combined with the weights $\mathbf{w}$ to produce outputs $y_k$ for each dimension $k$.}
        \label{fig:nn-multiclass}
    \end{subfigure}
    \caption{Computational graphs for perceptron architectures. (a) Standard perceptron. (b) Multi-output perceptron.}
    \label{fig:nn-combined}
\end{figure}

\cref{eq:linear-regression} can be expressed as a computational graph, as shown in \cref{fig:nn-perceptron}. This architecture is commonly referred to as the standard perceptron, the simplest of neural networks.

\subsection{Generalization of Linear Models}
Linear regression can be extended to more complex settings, such as multi-output models (e.g.\ multi-class classification or multidimensional regression), with $y_k$ representing the output for the $k$-th dimension, and $x_0$ often set to $1$ as a convention for including the bias term $w_{k0}$:
\begin{equation}\label{eq:multi-output}
    y_k(\mathbf{x}) = \sum_{j=0}^{d} w_{kj} x_j
\end{equation}
Such an architecture is known as a single layer perceptron, and can be visualized as shown in \cref{fig:nn-multiclass}.

This can be further generalized by introducing non-linear basis functions $\phi_j(\mathbf{x})$, transforming the input features $\mathbf{x}$ before the linear combination:
\begin{equation}
    y(\mathbf{x}) = \sum_{j=0}^{d} w_j \phi_j(\mathbf{x})
\end{equation}
These non-linear transformations allow models to approximate non-linear relationships in data. Due to this, the loss can no longer be minimized in closed-form so iterative optimization algorithms such as gradient descent are used \cite{bishopPatternRecognitionMachine2006}.
Conventionally, these were basis functions were predefined and hand-crated based on domain knowledge, but we next look at deep learning models that can learn these basis functions.

\subsection{Deep Learning}
Multi-layer perceptrons build upon the foundations of linear models but eliminate the need to predefine input transformations. This is done by learning the parametrized basis functions from the data. 

One way to generalize the linear model is by applying a non-linear activation function $g$ to each input in \cref{eq:multi-output}. For example:
\begin{equation}
y_k(\mathbf{x}) = g\left(\sum_{j=0}^{d} w_{kj} x_j \right).
\end{equation}
To achieve this, neural networks add multiple layers to the computational graph, with each layer applying a linear transformation followed by a non-linear activation. A simple architecture is shown in \cref{fig:simple-nn-architecture} (with the addition of non-linearities), illustrating how the standard perceptron is extended to deeper networks.

Due to the non-linearities and layered structure, neural networks have high-capacity hypothesis spaces, suited to approximate the complex mappings such as multidimensional data such as images. 

\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{images/simple_nn_architecture.pdf}
    \caption{An example trained fully connected neural network architecture with an input layer, two hidden layers, and an output layer. The input features are transformed through weights and biases, followed by a non-linear activation function in the first hidden layer. This process is repeated in the second hidden layer from outputs of the first (hidden) layer, producing the final outputs. The weights are represented by the lines connecting the neurons, with opacity indicating the weights' strength.}
    \label{fig:simple-nn-architecture}
\end{figure}

Activation functions are generally differentiable non-linear functions that introduce non-linearity into the network, allowing it to model complex relationships in the data. Common activation functions include the sigmoid function, that maps input values to a range between $0$ and $1$, useful to interpret probabilistically, and used in binary classification tasks. 
\begin{equation}
    S(x) = \frac{1}{1 + e^{-x}}
\end{equation}
Current state-of-the-art architectures often employ Rectified Linear Unit (ReLU) or its variants as the activation function, owing to their superior performance in training deep networks. ReLU defined as
\begin{equation}
    \text{ReLU}(x) = \max(0, x)
\end{equation}
sets negative inputs to zero while preserving positive inputs.

While ReLU is not differentiable at $x = 0$, it is widely favored due to its simplicity and effectiveness in training deep networks. Variants of ReLU, such as Leaky ReLU and Parametric ReLU have been introduced to address the issue of dead neurons, where neurons cease to learn during training. Leaky ReLU introduces a gradient for negative inputs
\begin{equation}
    \text{Leaky ReLU}(x) = \max(\alpha x, x)
\end{equation}
defined by a small constant $\alpha$. Parametric ReLU extends this concept by allowing the gradient to be learned during training.

Training a neural network involves two primary phases: the forward pass and the backward pass. During the forward pass, the input data is propagated through the network, layer by layer, to generate an output. Following this, the backward pass occurs, in which the error, defined as the difference between the predicted output and the true output, is propagated back through the network using the gradient of the loss function. This step is crucial for updating the weights of the network.
The training process utilizes optimization algorithms, such as Stochastic Gradient Descent (SGD) \cite{sutskeverImportanceInitializationMomentum2013} or Adam Optimization \cite{kingmaAdamMethodStochastic2017}, which iteratively adjust the network's weights to reduce the loss.

\section{Neural Networks for Image Restoration}
In the classical supervised-learning setting for image restoration, training assumes access to clean target images. Similar to \cref{sec:erm}, training can be formulated as
\begin{equation}
    \hat{h} = \argmin_{h \in \mathcal{H}} \frac{1}{\lvert \mathcal{S} \rvert} \sum_{(X, Y) \in \mathcal{S}} \ell(h(X; \theta), Y),
\end{equation}
where $\mathcal{S} \sim \mathcal{D}^n$ is the training set with images $\mathcal{S} = \{(X_1, Y_1), \dots, (X_n, Y_n)\}$ with $(X, Y)$  the noisy and clean target images.

\todo[author=zain,inline]{Finish the three subsections}
\subsection{Convolutional Neural Networks}
While Multilayered Perceptrons work great, for high dimensional data such as images, the number of parameters can become prohibitively large. 
"A CNN is a prior on the hypothesis space. In fact, it is such a strong prior that it puts 
 probability mass on any hypothesis that is not translationally equivariant (up to some details to do with image boundaries)"

\subsection{Autoencoder}
\cite{goodfellowDeepLearning2016}
\subsection{UNET}
\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{images/unet_architecture.pdf}
    \caption{}
    \label{fig:unet-architecture}
\end{figure}

\section{Noise2Noise Framework}
In many practical scenarios such as our \gls{MPES} data, obtaining the true clean image $Y$ is not feasible, although sufficiently high quality noisy data exists. In the instance when multiple noisy realizations of the same underlying signal are available, \citeauthor{lehtinenNoise2NoiseLearningImage2018} demonstrated that it is possible to train a neural network to learn a restoration mapping to the underlying clean signal using only noisy observations for both inputs and targets \cite{lehtinenNoise2NoiseLearningImage2018}, a framework known as \textit{Noise2Noise}. 

In the following sections, we see why Noise2Noise works and how it is applied to our image restoration problem.
\subsection{Point Estimation and Loss Functions}
We discussed in \cref{sec:generalization} that we aim to learn a hypothesis that generalizes to new data coming from the distribution $\mathcal{D}$. This can be done by minimizing the risk (\cref{eq:risk-min}). 

Consider an example point estimation problem where the objective is to find the scalar $\hat{x}$ that minimizes the expected deviation with respect to a set of observations $x_1, x_2, \dots, x_n$. This can be formalized as:
\begin{equation}
    \hat{x} = \argmin_{\hat{x}} \mathbb{E}[\ell(\hat{x}; x)]
\end{equation}
Different loss functions yield varying estimates based on the observations. For the case of $L_2$ loss, the minimization recovers the mean of the observations:
\begin{equation}\label{eq:l2-estimate}
    \hat{x} = \mathbb{E}[x]
\end{equation}
Similarly, for the $L_1$ loss, the estimate is the median of observations.

\subsection{Zero-Mean Noise}
Combining \cref{eq:risk} and \cref{eq:risk-min} can write the risk minimization problem as:
\begin{equation}
    \argmin_{h \in \mathcal{H}} \mathbb{E}_{x \sim \mathcal{D}} \left[\mathbb{E}_{y|x \sim \mathcal{D}} \left[ \ell(h(x; \theta), y) \right]\right]
\end{equation}
where using the law of total probability, the joint expectation $p(x,y)$ can be factorized to $p(x) \cdot p(y | x)$, and hence the expectation as well. This formulation is equivalent to solving the point estimation problem for each input sample separately. Due to this, the neural network inherits the properties of the loss function.

We established in \cref{eq:l2-estimate} that minimizing the $L_2$ loss recovers the mean of the observations. Now, consider the case where the training targets $y$ are corrupted with zero-mean noise, $\mathbb{E}[n] = 0$. The expectation remains unchanged i.e.\ $\mathbb{E}[y] = \mathbb{E}[x^\prime|y]$, with $x^\prime$ being the corrupted data. For example, we previously demonstrated Poisson noise being zero-mean  in \cref{sec:poisson-noise-model} (see \cref{eq:poisson-noise,eq:zero-mean-noise}), and hence, the $L_2$ loss could also recover, on expectation, the true value of a Poisson noise corrupted variable.

This principle can extend to neural network training as we already said that the network inherits the properties of the loss. When minimizing the risk, a neural network trained with zero-mean noise-corrupted targets will converge to the same optimal hypothesis as it would with clean targets. This holds true in the context of \gls{ERM} as well. With infinite training data, minimizing the empirical risk using noisy observations is mathematically equivalent to minimizing it with clean targets (\cref{eq:erm}).

\subsection{Noise2Noise Training for Finite Data}
Let us formalize the Noise2Noise training for a realistic case of finite examples. Consider a training set $\mathcal{S}^\prime = \{(X_1, X_1^\prime), \dots, (X_n, X_n^\prime)\}$ where $\mathcal{S}^\prime \sim \mathcal{D}^n$, each pair $(X, X^\prime)$ independent noisy realizations of the same underlying signal $y \sim \mathcal{D}$. Using the $L_2$ loss, we can redefine \gls{ERM} formulation from \cref{sec:erm} as
\begin{equation}\label{eq:erm-noise2noise}
    \hat{h} = \argmin_{h \in \mathcal{H}} \frac{1}{\lvert \mathcal{S^\prime} \rvert} \sum_{(X, X^\prime) \in \mathcal{S}^\prime} (h(X; \theta) - X^\prime)^2
\end{equation}
giving us a hypothesis $\hat{h}$ that minimizes the training error using only noisy observations for both inputs and targets.

For finite data, the quality of the estimate depends on the variance of the noise in the targets, divided by the number of samples $N$ \cite[supplementary~material]{lehtinenNoise2NoiseLearningImage2018}. This means that increasing the dataset size reduces the variance of the estimate, bringing it closer to the hypothesis had we minimized with clean targets. For image data, $N$ corresponds to the total number of scalar components\footnote{Number of images $n$ x number of voxels per image x number of color channels.} across the dataset, so having more voxels and data effectively brings us closer to the infinite data case.

\subsection{Regularization through Noisy Targets}
It is important to note that the earlier discussion assumed that learning with clean targets is inherently successful, while in fact, as we discussed before, \gls{ERM} is prone to overfit (generalizes poorly) in high-capacity hypothesis spaces. Using noisy targets actually has the unexpected benefit to act as a form of regularization. The noise in the targets perturbs the training objective, preventing the network from relying excessively on precise input-output mappings. For image restoration tasks, this can lead to better generalization, as the network focuses on recovering the underlying clean signal structure rather than spurious details in the data.

\section{Training an MPES denoiser}
As seen in \cref{sec:image-formation}, the experimental setup (refer to \cref{section:hextof,section:dld}) for \gls{MPES} puts us in a unique position of having access to multiple noisy realizations of the same underlying signal. This allows us to generate training data pairs where both the input and target are noisy realizations of the same underlying signal.

Using \cref{eq:erm-noise2noise}

If we generate images with overlapping subsets, the formed images are not independent, as the same events could be present in multiple images. A simple example would be to generate image an image with \num{e6} counts and other with twice as many counts, where the former is a subset of the latter. This is a limitation of the method, but it is a reasonable approximation for the purpose of this study.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{images/training_flowchart.pdf}
    \caption{Flowchart illustrating the input-target dataset pairs used for neural network training, derived from subsets of the \gls{GrIr} dataset. A total of \num{34} unique noisy subsets are represented, with each dataset corresponding to one blue box in the chart. The different combinations shown in the chart generate \num{51} input-target pairs across various counts, including \numlist{1e6;2e6;4e6;8e6;1.6e7;3.2e7;4.8e7;9.6e7}. Notably, the \num{4.8e7} and \num{9.6e7} counts serve as the target datasets, with \num{9.6e7} additionally functioning as the target for the \num{4.8e7} datasets.}
    \label{fig:training-data}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{images/training_3d_patch_example.pdf}
    \caption{}
    % \label{fig:images-noisy-denoised-training}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{images/noisy_denoised_3d.pdf}
    \caption{}
    % \label{fig:images-noisy-denoised-training}
\end{figure}


\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{images/images_noisy_denoised_with_target.pdf}
    \caption{Noisy and denoised \gls{ky}-\gls{kx} cuts with window size $w=1$ shown for \gls{GrIr} dataset. Each column corresponds to \numlist{1e6;2e6;4e6;8e6;1.6e7;3.2e7;1.86e8} counts, respectively; where the last column is the target image with $w=1$ in row 1 and $w=15$ in row 2. Below \num{8e6} counts, none of the features are discernible for noisy images, while the denoised images show clear features similar to the target.}
    \label{fig:images-noisy-denoised-training}
\end{figure}


\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{images/training_progress_example2.pdf}
    \caption{}
    \label{fig:training-progress-example}
\end{figure}


\begin{figure}
    \centering

    \begin{subfigure}[b]{0.7\linewidth}
        \centering
        \includegraphics[width=1\linewidth]{images/loss_training_val.pdf}
        \caption{}
        \label{fig:loss-training-val}
    \end{subfigure}

    \begin{subfigure}[b]{0.7\linewidth}
        \centering
        \includegraphics[width=1\linewidth]{images/ssim_training_val.pdf}
        \caption{}
        \label{fig:ssim-training-val}
    \end{subfigure}

    \caption{Training and validation loss and \gls{SSIM} scores for the neural network model trained on the \gls{GrIr} dataset.}
    \label{fig:loss-ssim-training-val}
\end{figure}

\section{Testing}
\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{images/nn_gdw_msssim.pdf}
    \caption{\num{1e6} counts take approximately \qty{11.5}{min} to acquire. \gls{GdW} dataset.}
    \label{fig:gdw-test-metirc}
\end{figure}


\begin{figure}
    \centering
    \begin{subfigure}[b]{1\linewidth}
        \centering
        \includegraphics[width=1\linewidth]{images/nn_denoised_ex_w_1.pdf}
        \caption{}
        \label{fig:nn-denoised-ex-w-1}
    \end{subfigure}

    \begin{subfigure}[b]{1\linewidth}
        \centering
        \includegraphics[width=1\linewidth]{images/nn_denoised_ex_w_20.pdf}
        \caption{\num{4e6} count dataset. The denoising performance leaves room for improvement, using the adjusted optimal $\sigma_{\text{o}}\approx0.4$.}
        \label{fig:nn-denoised-ex-w-20}
    \end{subfigure}
    \caption{}
    \label{fig:nn-denoised-ex-w}
\end{figure}

\begin{figure}
    \centering
    \begin{subfigure}[b]{1\linewidth}
        \centering
        \includegraphics[width=1\linewidth]{images/nn_denoised_xy_w_1.pdf}
        \caption{}
        \label{fig:nn-denoised-xy-w-1}
    \end{subfigure}

    \begin{subfigure}[b]{1\linewidth}
        \centering
        \includegraphics[width=1\linewidth]{images/nn_denoised_xy_w_20.pdf}
        \caption{}
        \label{fig:nn-denoised-xy-w-20}
    \end{subfigure}
    \caption{}
    \label{fig:nn-denoised-xy-w}
\end{figure}