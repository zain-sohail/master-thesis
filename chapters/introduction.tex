Conjecturing hypotheses based on observations has always been an important aspect of the scientific methodology. Especially in natural sciences, where the aim is to describe natural phenomena, the role of observations can not be overstated. 

% Different from mathematics or other sciences which are based on deductive reasoning, empirical sciences are based on inductive reasoning, many times making use of its formalized form, statistical inference.

Trying to make sense of the observations is the.

After formulating a hypothesis, an experiment is designed to test it. From the evidence gathered through the experiment and through the aid of statistical principles, these hypotheses can be accepted (or rejected) with a defined level of confidence.

Hence, science is inherently linked to data. Data science helps us to formally handle this data, regardless of the domain. We want testable outcomes

With the dimensionality and size of data increasing, more sophisticated tools are necessary. 

Especially in the field of neuroscience where experiments can not be performed easily or directly, a lot of statistical tools are employed.

In modern days, a paradigm shift has occurred where instead of trying to explicitly model the system we are trying to learn about, 
has also brought revolutions such as machine learning where we don't need to know the model itself and basically make the machine learn the non-linear model. This doesn't explain the process but allows us to perform for example inference. Learning is a sort of metamodel, where we don't need to know the theory itself but can still make predictions.


Expensive aspect! People don't use math methods first.
Then we go into denoising 

With the necessity to acquire data in so many dimensions, because we have low electron counts.

\gls{fel} \gls{undulator} \gls{train} \gls{pulse} \gls{sase}