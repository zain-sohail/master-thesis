Conjecturing hypotheses based on observations has always been an important aspect of the scientific methodology. Especially in natural sciences, where the aim is to describe natural phenomena, the role of observations can not be overstated. Unlike mathematics, which is grounded in deductive reasoning, empirical sciences rely heavily on inductive reasoning, and frequently employ statistical inference to draw conclusions from experimental data.

With this sort of problem appearing in all of the sciences, an overarching field of study in
the field of data science has emerged. Data science is the study of data, where data is understood as information that is collected, processed, and analyzed. The goal of data science is to extract knowledge from data, and to use this knowledge to make decisions or predictions.
\cite{ackermannOperationFreeelectronLaser2007}
After formulating a hypothesis, an experiment is designed to test it. From the evidence gathered through the experiment and through the aid of statistical principles, these hypotheses can be accepted (or rejected) with a defined level of confidence.

Hence, science is inherently linked to data. Data science helps us to formally handle this data, regardless of the domain. We want testable outcomes

With the dimensionality and size of data increasing, more sophisticated tools are necessary. 

Especially in the field of neuroscience where experiments can not be performed easily or directly, a lot of statistical tools are employed.

In modern days, a paradigm shift has occurred where instead of trying to explicitly model the system we are trying to learn about, 
has also brought revolutions such as machine learning where we don't need to know the model itself and basically make the machine learn the non-linear model. This doesn't explain the process but allows us to perform for example inference. Learning is a sort of metamodel, where we don't need to know the theory itself but can still make predictions.


Expensive aspect is to buy new equipment. People don't use math methods first.
Then we go into denoising 

With the necessity to acquire data in so many dimensions, because we have low electron counts.

\gls{fel} \gls{undulator} \gls{train} \gls{pulse} \gls{sase}
