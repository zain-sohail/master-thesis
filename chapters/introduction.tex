Scientific methodology, especially in empirical fields such as physics, fundamentally rely on observations as the basis to understand the principles of nature. The act of measurement, however, is seldom free from ambiguity. For instance, due to environmental factors, instrument limitations, or the intrinsic uncertainties of quantum phenomena. 

Ingeniously designed experiments, sophisticated detection schemes, and controlled environments--such as ultra-low temperatures--all contribute to minimizing external noise and improving the quality of data collected. However, as experiments push the boundaries of scale and precision, the limitations imposed by quantum mechanics, such as uncertainty and fluctuations, remain unavoidable.

$\dots$

% In modern days, a paradigm shift has occurred where instead of trying to explicitly model the system we are trying to learn about, 
% has also brought revolutions such as machine learning where we don't need to know the model itself and basically make the machine learn the non-linear model. This doesn't explain the process but allows us to perform for example inference. Learning is a sort of metamodel, where we don't need to know the theory itself but can still make predictions.


% Expensive aspect is to buy new equipment. People don't use math methods first.
% Then we go into denoising 

% With the necessity to acquire data in so many dimensions, because we have low electron counts.


$\dots$

This thesis can serve as an introduction to readers, such as from a background of computer science, interested to learn about the fundamentals of photoemission spectroscopy, its motivation and challenges and how statistical and machine learning techniques can be applied in this domain. An attempt is made to provide a thorough foundation to learning algorithms in \cref{ch:deep_learning}, as the field is often treated as black box.

$\dots$