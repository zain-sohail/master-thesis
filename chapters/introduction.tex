Scientific methodology, especially in empirical fields such as physics, fundamentally rely on observations as the basis to understand the principles of nature. The act of measurement, however, is seldom free from ambiguity. For instance, due to environmental factors, instrument limitations, or the intrinsic uncertainties of quantum phenomena. 

Ingeniously designed experiments, sophisticated detection schemes, and controlled environments--such as ultra-low temperatures--all contribute to minimizing external noise and improving the quality of data collected. However, as experiments push the boundaries of scale and precision, the limitations imposed by quantum mechanics, such as uncertainty and fluctuations, remain unavoidable.

$\dots$

% In modern days, a paradigm shift has occurred where instead of trying to explicitly model the system we are trying to learn about, 
% has also brought revolutions such as machine learning where we don't need to know the model itself and basically make the machine learn the non-linear model. This doesn't explain the process but allows us to perform for example inference. Learning is a sort of metamodel, where we don't need to know the theory itself but can still make predictions.


% Expensive aspect is to buy new equipment. People don't use math methods first.
% Then we go into denoising 

% With the necessity to acquire data in so many dimensions, because we have low electron counts.

Let us first discuss why the observations which are inherently stochastic converge to the true distribution as the number of observations increases. This result is so foundational to experimental science that it is rarely stated explicitly.

Consider a sequence of \gls{iid} \glspl{rv} $X_1, X_2, \dots, X_n$ defined on the probability space $(\Omega, \mathcal{A}, P)$ (see \cref{section:probability-notation}), representing the number of detected events in different intervals. The true mean or expected value of these \gls{rv} can be defined as $\mu = \mathbb{E}(X_1)$
\footnote{Since the \gls{rv} are \gls{iid}, the expected value $\mu$ of each $X_i$ is the same.} 
and variance as $\sigma^2 = Var(X_1)$.

Formally, by the \gls{LLN}\footnote{Specifically, the weak \gls{LLN}, which also has a stronger variant proving \gls{as} convergence.}, the sample mean $\bar{X_n} = \frac{1}{n} \sum_{i=1}^{n} X_i$ converges in probability to the expected value $\mu$ as $n \to \infty$ \cite{fellerIntroductionProbabilityTheory1968}:
\begin{equation}
    \lim_{n \to \infty} P(|\bar{X_n} - \mu| > \epsilon) = 0
\end{equation}

Hence, experimental scientists have an assurance that after collecting enough data, the observed mean will approximate the true value of the physical system under study.

The rate of convergence to $\bar{X_n}$ is of practical importance, as it indicates how quickly the sample statistics approximate the true distribution:
\begin{equation}
    \left|\bar{X_n} - \mu\right| = \mathcal{O}\left(\frac{1}{\sqrt{n}}\right)
\end{equation}

indicating that the relative fluctuations decrease with an increased number of observations $n$.  Since $n \propto T$, increasing $T$ reduces the sample mean fluctuations. While convergence rate described by $\mathcal{O}\left(\frac{1}{\sqrt{n}}\right)$
\footnote{Big O notation, denoted as $\mathcal{O}(g(n))$, describes an upper bound on the time complexity of an algorithm or the growth rate of a function.} is a fundamental result, the constant that accompanies this rate is distribution dependent, and can affect the convergence rate.

In experiments such as \gls{MPES}, the number of detected events is often limited due to a variety of reasons, either because the events themselves are rare, such as when probing certain non-linear processes or material dynamics, or due to limitations in the detection scheme, such as the detector efficiency or range. This limitation becomes especially relevant when studying transient processes, where system changes happen on ultrashort timescales, constraining the data acquisition time-window.  Added to this the multidimensional nature of the experiment, where increasing the dimensionality leads to the sample space becoming sparser, and exponentially more data is needed to fill the space.

This results in low number of counts, insufficient to accurately estimate the true distribution. While increasing the acquisition times would reduce these fluctuations, in experiments with limited \glspl{beamtime} such as at large scale facilities like \glspl{FEL} or synchrotrons, this is not possible. Therefore, techniques that can extract the maximum information from the limited data are essential; techniques that access correlations and structures in the multidimensional space to improve the estimation of the true distribution.

$\dots$

In this thesis, when we refer to noise, we are specifically addressing the inherent fluctuations in the experimental data due to its stochastic nature. This noise is not simply an artifact of measurement, but a fundamental characteristic of the processes involved in \gls{MPES}. The observed data in such experiments are governed by random fluctuations arising from the probabilistic nature of photon detection and electron emission events.

To provide context, experimental observations—such as those in \gls{MPES} modeled as inherently stochastic processes, where the data consist of independent and identically distributed (i.i.d.) random variables. These random variables represent the detected events across different intervals, with their expected values converging to the true mean as the number of observations increases, as described by the law of large numbers (LLN). However, in \gls{MPES} experiments, constraints such as limited beamtime, detector efficiency, and the multidimensional nature of the data often limit the number of detectable events, resulting in low counts that are insufficient to accurately estimate the true distribution. This introduces challenges in reducing the fluctuations that stem from limited data.

Throughout this work, the term “noise” thus refers to these inherent fluctuations, which affect our ability to directly measure the underlying distribution of physical processes. This thesis explores techniques that can mitigate the impact of such noise by leveraging correlations and structures within the data, particularly in the context of image denoising and reconstruction. These techniques, whether classical or deep learning-based, are critical for extracting the maximum amount of information from sparse and noisy datasets, especially when experimental constraints prevent the collection of large numbers of observations.

In summary, the treatment of noise in this thesis extends beyond simple measurement error; it encompasses the inherent statistical uncertainty in \gls{MPES} data. By developing and applying sophisticated denoising methods, we aim to improve the estimation of true distributions despite the limitations of data collection, providing better insight into the underlying physical phenomena.

$\dots$

The present thesis is concerned with the denoising and reconstruction of multidimensional images generated from \gls{MPES} experiments, a challenging problem at the intersection of experimental physics and computational imaging. Specifically, this work focuses on developing methods for improving the quality of noisy photoemission data, which has relevance for both experimental physicists working with advanced light sources, such as \glspl{PES}, and data scientists interested in state-of-the-art image restoration techniques.

The thesis begins by introducing the fundamental concepts of \gls{PES} in \cref{ch:pes} and provides an overview of the experimental setup utilized in multidimensional photoemission studies. A detailed description of the light sources used, particularly FELs, is provided, along with an explanation of their relevance in generating the high-energy, ultrafast pulses necessary for such experiments.

Following this, the thesis discusses the image corruption model relevant to \gls{MPES} data, where we briefly survey classical image denoising techniques before focusing on the application of \gls{BM3D}, a widely-used algorithm in image denoising. A discussion on noise modeling is provided, with particular emphasis on Poisson noise, which is commonly assumed in photon-counting applications. To address this, the Anscombe transform is introduced, along with its inversion, to stabilize the variance of the noisy data. This chapter evaluates the applicability of the Anscombe transform to \gls{MPES} data and concludes that it does not lead to significant improvements, suggesting that the noise distribution may not strictly follow a Poissonian model.

The thesis then shifts focus to the statistical characterization of photoelectron counts in the context of \gls{FEL} sources, where the photoelectron emission process is described using the Cox process, a generalization of Poisson processes that accounts for the non-Poissonian nature of \gls{FEL} light, which follows a negative binomial distribution. Given the uncertainties surrounding the exact noise characteristics in the data, the thesis explores how single-event measurements can be used to estimate the temporal distribution of photoelectron events and better understand the underlying process.

To address the challenges posed by the non-Poissonian nature of the data and the complexity of the multidimensional datasets, the thesis progresses toward a deep learning-based approach. Specifically, we develop a 3D convolutional neural network (UNet3D) to denoise the \gls{MPES} data. The network is trained using the Noise2Noise paradigm, where both the input and target data are noisy, bypassing the need for clean reference data. This method offers a promising solution for denoising in the case of highly complex and multidimensional data, such as that produced in \gls{MPES} experiments.

In summary, this thesis presents a multifaceted exploration of image denoising techniques in the context of \gls{MPES}, bridging experimental physics and modern machine learning approaches. It provides valuable insights for physicists looking to improve data quality in photoemission experiments, while also offering a novel case study for data scientists working on image restoration techniques, particularly those involving deep learning methods for noisy datasets. Each chapter addresses distinct but interconnected aspects of the problem, culminating in a comprehensive approach to handling the challenges of noise in high-dimensional experimental data.

This thesis aims to bridge the gap between experimental physics and computational techniques, providing an accessible introduction to \gls{PES} for readers from diverse backgrounds, including data science and machine learning. While the core focus is on image denoising and restoration in \gls{MPES} data, the thesis also offers a broader perspective on how statistical and machine learning techniques can be applied to experimental data processing.

The work is structured to cater to two distinct audiences: first, physicists who may benefit from an understanding of modern image processing techniques; and second, data scientists interested in learning how machine learning can be applied to experimental data. For those unfamiliar with PES, the thesis provides a detailed introduction to the fundamental principles of the technique, its motivation, and the challenges posed by noise in the data, making it an invaluable resource for data scientists who seek to explore this area. Conversely, for those with a background in experimental physics, it demonstrates the potential of machine learning and statistical methods to address practical issues in data quality and interpretation.

A particular effort has been made to demystify the machine learning models used throughout this work, especially in the deep learning-based approaches. While the field is often treated as a “black box,” this thesis dedicates substantial attention to explaining the foundations of learning algorithms and their applications, particularly in \cref{ch:deep_learning}. This chapter serves as an in-depth introduction to key concepts in statistical learning and neural networks, offering insights into how these tools can be understood and applied systematically, rather than being seen as purely empirical methods.

In summary, this thesis not only contributes to the development of new denoising techniques for \gls{MPES} but also aims to serve as a comprehensive introduction to both \gls{PES} and the application of machine learning in this domain. By combining theoretical insights with practical applications, it is hoped that this work will be a useful resource for both physicists and data scientists interested in these interconnected fields.
$\dots$