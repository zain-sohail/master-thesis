Scientific methodology, especially in empirical fields such as physics, fundamentally rely on observations as the basis to understand the principles of nature. The act of measurement, however, is seldom free from ambiguity. Ingeniously designed experiments, sophisticated detection schemes, and controlled environments--such as ultra-low temperatures or precisely engineered detection schemes--all contribute to minimizing external noise and improving the quality of data collected. However, as experiments push the boundaries of scale and precision, the limitations imposed by quantum mechanics, such as uncertainty and fluctuations, remain unavoidable \cite{heisenbergPhysicalPrinciplesQuantum2009,sakuraiModernQuantumMechanics2020,binneyPhysicsQuantumMechanics2014}.

One important area where such challenges are evident is \gls{PES}. \Gls{PES} is used to study the electronic structure of materials by measuring the energy (and momentum) of emitted electrons from a sample, irradiated by a light source \cite{cardonaGeneralPrinciples1978}, allowing to understand material properties at an atomic level. This makes \gls{PES} an invaluable tool in modern material science. 

Importantly, this entire process of photon absorption and electron emission is inherently probabilistic. When a material is irradiated, each photon has a certain probability of interacting with electrons in the material, and the electrons have a certain probability of being emitted, thus introducing stochastic variability in the measurements.

Despite this stochastic nature, experimentalist can rely on the fundamental law in probability theory, the law of large numbers\footnote{Refer to \cref{section:law-of-large-numbers}}, guaranteeing that the observed data converges to the true distribution as the number of observations increases. However, while adding more data reduces \gls{noise}, the improvement occurs at a diminishing rate. This is highlighted by the rate of convergence being proportional to the inverse square root of the number of observations\footnote{$\mathcal{O}(g(n))$, describes an upper bound on the growth rate of a function.}, $\mathcal{O}(1/\sqrt{n})$. This implies that in experiments with limited time or resources, it is often impractical to collect enough data to achieve the desired precision.

\Gls{MPES} extends \gls{PES} by measuring across multiple dimensions, such as time, spin, probe energy etc., allowing researchers to capture a more comprehensive view of the electronic structure and dynamics of materials. The increase in dimensionality, however, necessitates exponentially more events to fill the sample space. This becomes particularly problematic when event detection is constrained by rare phenomena, specific measurement schemes \cite{maklarQuantitativeComparisonTimeflight2020}, space-charge effect \cite{schonhenseMultidimensionalPhotoemissionSpectroscopy2018}, or short timescales of transient events, constraining the data acquisition time-window. The result is a low number of counts, insufficient to accurately estimate the \gls{latent} distribution. 

While increasing the acquisition times would reduce these fluctuations, experiments at large-scale facilities, like \gls{FEL} or synchrotrons, are often limited by strict \gls{beamtime} allocations. Therefore, techniques that can extract the maximum information from the limited data are essential; techniques that access correlations and structures in the multidimensional space to improve the estimation of the latent distribution.

The present thesis is hence concerned with the estimation of the latent multidimensional images from incomplete observations generated by the \gls{MPES} experiments, a complex problem at the intersection of experimental physics and data science. The primary focus is on developing methods to enhance the quality of noisy photoemission data, a challenge that holds significance for experimental physicists working with advanced light sources, such as \glspl{FEL}, and for data scientists interested in cutting-edge image restoration techniques. Another focus of this work is to examine the statistical properties of photoemitted electrons, an interesting study in its own right, but one that can also aid in identifying characteristics informing more effective restoration approaches. Additionally, emphasis is placed on explaining the instrumentation, as these details are critical to understanding how the data is generated.

In \cref{ch:pes}, an overview of the fundamental concepts of \gls{PES} is provided. We discuss the time-resolved variant of \gls{PES} with a special emphasis placed on the light sources (such as lasers and \glspl{FEL}) used, the \gls{HEXTOF} instrument, and the segmented \gls{DLD}, as these elements are crucial in understanding how data is generated. This experimental scheme, being at the forefront of experimental physics, introduces significant challenges in terms of data acquisition, complicating the collection of sufficient data in multidimensional experiments. This chapter further addresses how these experimental constraints impact the quality and quantity of data obtained, setting the stage for the image restoration techniques developed later in the thesis.

Following this introduction, in \cref{ch:denoising}, the thesis moves into an exploration of the image corruption model and reviews  classical image denoising techniques such as Wiener filtering and \gls{NLM}, culminating in a focus on the application of \gls{BM3D}. Additionally, Poisson noise modeling is discussed, commonly assumed in event-counting experiments. The Anscombe transform, and its inversion are then introduced as techniques to stabilize the variance of noisy data, allowing the usage of Gaussian noise models and denoising techniques designed for such models.

The subsequent chapter, \cref{ch:datasets_bm3d}, describes the specific datasets used throughout the thesis. We look at the  process by which the single-event data is transformed into multidimensional images, how noisy realizations are generated, and the metrics employed to assess image quality. Finally, we evaluate the performance of \gls{BM3D} with and without the Anscombe transform on these datasets. This evaluation involves optimizing hyperparameters and then examining how denoising effectiveness varies with electron counts, allowing us to understand the denoising effectiveness in low-count scenarios.

Recognizing the limitations of the Poisson noise model and classical denoising techniques, the thesis shifts toward a statistical characterization of photoemission events in \cref{ch:pes-statistics}, particularly in the context of \gls{FEL} light sources. The photoelectron emission process is described using the doubly stochastic Poisson point process, a generalization of the \gls{PPP}, which accounts for the non-Poissonian counting statistics of \gls{FEL} light. We then explore how single-event measurements can be utilized to estimate the temporal distribution of photoemission events, providing deeper insight into the underlying data generation process.

With the complexity of high-dimensional datasets and complex counting statistics, we employ a deep learning-based approach in \cref{ch:deep_learning}. Specifically, a 3D \gls{CNN}, the UNet3D architecture, is trained to denoise \gls{MPES} data. The training follows the \gls{noise2noise} paradigm, utilizing multiple noisy realizations derived from the single-event data. This novel method offers a promising solution for denoising complex, multidimensional data generated in \gls{MPES} experiments. Furthermore, considerable effort is made to demystify learning based models. Key concepts in machine learning and neural networks are presented in-depth, providing a clearer understanding of how these tools can be effectively applied in experimental data processing.

In summary, this thesis not only aims to develop denoising techniques and model the data-generating process for \gls{MPES} data but also to serve as an introduction to both \gls{PES} and the application of denoising, both classical and learning-based, in this domain. By combining these topics, it is hoped that this work will be a useful resource for both physicists and data scientists interested in these interconnected fields.