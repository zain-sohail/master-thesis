This thesis has presented an investigation into image denoising methodologies within the scope of \gls{MPES} data, addressing inherent limitations posed by the probabilistic nature of photoemission events, the multidimensional sample spaces, and compounded by the experimental constraints such as space-charge effect, sample degradation from radiation damage and practical constraints such as limited  beamtime allocations. These challenges are particularly pronounced in modern \gls{MPES} experiments leveraging cutting-edge light sources such as \glspl{FEL} and \gls{HHG} lasers that present unique statistical and operational challenges.

The work initially considered the \glsxtrfull{BM3D}, with and without variance stabilization through the Anscombe transform. While, these methods performed quite well for high-count data, the performance degrades considerably in the low-count regimes, reflecting their inability to exploit the multidimensional correlations intrinsic to the MPS data. While, perceptual improvements were observed with the Anscombe transform compared to just using \gls{BM3D}, the lack of clean target datasets rendered quantitative assessment of this effect challenging. We showed that \gls{MSSSIM} is a better metric with a noisy target, but more domain-specific evaluation metrics, tailored to the physics of \gls{MPES}, such as line shape analysis of momentum and energy distribution curves, could prove more beneficial.

This work has also contributed to the statistical analysis of photoemitted electrons, with an emphasis on electron counting distributions under FEL illumination. Our analysis confirmed that the stochastic SASE process is described by a doubly stochastic Poisson point process giving rise to NB counting statistics. The results presented here underscore the necessity of revising the commonly assumed Poisson model  within photoemission data and highlights the necessity of reevaluating models for variance stabilization and noise assessment in \gls{MPES}, especially when utilizing nonlinear \gls{HHG} sources or \glspl{FEL}. These insights, grounded in both theoretical analysis and experimental data, extend beyond \gls{MPES} to other disciplines reliant on similar photon-based counting experiments. \todo{add specially designed experiment for statistics measurment.}

In order to overcome the constraints of traditional techniques within the extremely low-count regime, this thesis focused on deep learning methodologies, specifically the UNet3D architecture trained within the self-supervised learning paradigm, Noise2Noise. Exploiting the  correlations intrinsic to multidimensional data and using single-event datasets in order to create training examples, this model achieves  remarkable improvements in denoising in sparse \gls{MPES} datasets. We showed that for as little as \num{1e-3} average counts per voxel, the model predictions (corresponding to a \num{1e6} dataset) exceed higher count \gls{BM3D}-denoised images.

Advancements in variance stabilization techniques and their inversions for negative binomial noise, paired with an algorithm such as BM4D, leveraging 3D data, could prove useful when faced with no training data. The deep learning training would benefit from retraining using the \gls{MSSSIM} metric of evaluation and if possible, clean images. It would also be interesting to incorporate other experimental setup datasets, and also to try pretrained networks, or try unsupervised approaches such as Deep Image Prior and Noise2Void.

The potential impact of this work would enable more efficient data acquisition and analysis, the methods developed here streamlining experiments at large-scale facilities such as \glspl{FEL} or table-top laboratory setups, optimizing beamtime usage and expanding the scope of feasible studies. Furthermore, the insights into counting statistics and denoising techniques may be applicable to other counting experiments, such as x-ray diffraction, scattering, and other multidimensional spectroscopies. These methodologies also hold promise for applications in other fields facing difficulties with sparse, noisy datasets, such as medical imaging, astronomy, and particle physics.

Despite its successes, this work faced several limitations, including restricted test datasets and the lack of clean ground truths for metric evaluation. Addressing these limitations will be a priority for future studies, which could explore validation with domain-specific metrics, and expanded datasets encompassing a wider range of noise levels and experimental conditions. 

Beyond its methodological contributions, this thesis can serve as a valuable resource for experimental physicists and data scientists alike, providing detailed descriptions of the \gls{MPES} process, instrumentation, and theoretical foundations and discussion of deep learning.


% so this thesis has presented a comprehensive investigation into image denoising methodologies within the scope of, but not limited to MPES data. I presented  thorough introduction to the experimental technique, which is at the forefront of scientific avenue, but with challenges coming from the physical laws of nature; the probabilistic nature of pes. The inherent noise in light source, shot noise, is discussed. Difficulties due to  filling of multidimensional sample space, due to the space charge effect in \gls{trPES} are discussed to set the stage. limitations of beamtime allocations restricting the number of measurements that can be taken as well. 
% we talk about the complex segemented delay line detector that is used, and how this affects event counting and the data that is generated.

% specifically in \cref{ch:pes-statistics}, we dove deep into counting statistics of photoemitted electrons. We refer to extensive literature that contributed to the topic of photoelectron statistics and also show through our data that it follows the hypotheiszed results. Namely that the SASE process of FEL is following a doubly stochastic poisson point process and the counting statistics are negative binomial. The poisson assumption is valid only for coherent laser light sources, and not even for certain as the non-linear HHG process light sources (used in mpes actually) have recently been shown to have varying statistics by \citeauthor{gorlachQuantumopticalNatureHigh2020}. but these results are presented after an introduction to our experimental setup, which is used an FEL source. However, the findings here are not restrictive to this. Just interesting to note that one should be careful when assuming poisson statistics in their data.

% we have analyzed the usage of metrics, since we dont have clean targets to compare against. we decided to use \gls{MSSSIM} as it provided best distinction between noisy and denoised data but ideal situation for future work would be to have a clean target to compare against, or a more domain specific metric such as Line shape analysis, mdcs, commonly used derivative based techinque in arpes etc.
% But still using this metric, we show that the metric reports similar performance for both bm3d with and without variance stabilization through anscombe but perceptually, the anscombe leads to a bit more better results, due to the stabilization of variance, and hence not excessive denoising for high count data, compared to the lower count data.

% for future work, vsts for nb noise can be discussed but in this thesis we discussed results already published regarding vsts and optimal inverses of poisson noise as that is the common assumption, which we learnt to be false, though this is also in published research already about photoelectrons statistics by mandel in general and specifically by fel creators \citeauthor{saldinStatisticalPropertiesRadiation1998}

% lastly, key contribution in extremely low count statistics (talking an average voxel count of \num{1e-3} which correponds to a \num{1e6} dataset) is the use of deep learning, specifically unet3d, trained on noise2noise paradigm. this approach accesses the spatial correlations in the data and is able to denoise the data effectively. we do this training with limitless access to training data as we have access to event data that we can bin independent realizations from. this is a promising approach for future work, as it can be extended to other datasets and other denoising tasks.
% as mentioned before, we have limited data, and the test dataset we employed had max counts of \num{2.21e6}, which corresponds to about half that count in the image itself (since we filter some ranges out). this shows perceptually amazing results for extremely low counts but since we train on the lower counts, it doesn't perform as well on higher counts. moreover, a better validation set is needed to validate the model, and currently we used ssim (as this was done before) for metric but in future msssim as metric is planned. The effect of training set should also be investigated, as we used significant amount of data but this also requiers a very long training process. we should also inspect the effect of noise levels on the training process, as we used really high noise levels (very low counts as said). 

% A big contribution to describe everything that has been used is made, such as pes process, hhg, fel, momentum microscopes, such as the deep learning algorithms and it's basis from statistical learning paradigms so that this can be used as a reference for future work.

% the research has potential to streamline the MPES data acquisition process at table-top/laboratory sources as well as large-scale facilities like FEL FLASH. By utilizing our method in future studies, researchers will be able to efficiently optimize acquisition parameters; thus, significant beamtime could be conserved, or an existing beamtime budget could be used more effectively, allowing for the exploration of a broader parameter space.

% this work can also be used as a direction to others using fel facilities doing other counting experiments, such as x-ray diffraction, x-ray scattering, etc. as the counting statistics are similar, and the description we provided could aid in exploring this for such researchers. you can suggest other fields where such would be valuable.

