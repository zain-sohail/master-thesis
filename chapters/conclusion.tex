This thesis has presented an investigation into image denoising methodologies within the scope of \gls{MPES} data, addressing inherent limitations posed by the probabilistic nature of photoemission events, the multidimensional sample spaces, and compounded by the experimental constraints such as space-charge effect, sample degradation from radiation damage and practical constraints such as limited  beamtime allocations. These challenges are particularly pronounced in modern \gls{MPES} experiments leveraging light sources such as \glspl{FEL} and \gls{HHG} lasers that present unique statistical and operational challenges.

The work initially considered the \glsxtrfull{BM3D}, with and without variance stabilization through the Anscombe transform. While these methods performed quite well for high-count data, the performance degrades considerably in the low-count regimes. Moreover, reflecting the inability to exploit the multidimensional correlations of the MPES data. Although perceptual improvements were observed with the Anscombe transform compared to solely using \gls{BM3D}, the lack of clean target datasets rendered quantitative assessment of this effect challenging. We showed that \gls{MSSSIM} is a better evaluation metric, compared to other reference-based metrics, when the target image is noisy. However, domain-specific evaluation metrics, tailored to the physics of \gls{MPES} such as line shape analysis of momentum and energy distribution curves, could prove more beneficial.


This work has also contributed to the statistical analysis of photoemitted electrons, with an emphasis on electron counting distributions under FEL illumination. Our analysis confirmed that the stochastic SASE process is described by a doubly stochastic Poisson point process giving rise to negative binomial counting statistics. The results presented here highlights the importance of revising the commonly assumed Poisson model within photoemission data and reevaluating models for variance stabilization and noise assessment in \gls{MPES}, especially when utilizing nonlinear \gls{HHG} sources or \glspl{FEL}. These insights can be extended beyond \gls{MPES} to other disciplines reliant on similar photon-based counting experiments.

In order to overcome the constraints of traditional techniques within the extremely low-count regime, this thesis focused on deep learning methodologies, specifically the UNET3D architecture trained within the self-supervised learning paradigm, Noise2Noise. Exploiting the correlations intrinsic to multidimensional data and using single-event datasets in order to create training examples, this model achieves remarkable improvements in denoising in sparse \gls{MPES} datasets. We showed that for as little as \num{1e-3} average-counts per voxel, the model predictions (corresponding to a \num{1e6} dataset) exceed higher count \gls{BM3D}-denoised images.

Advancements in variance stabilization techniques and their inversions for \gls{NB} noise, paired with an algorithm such as BM4D, leveraging 3D data, could prove useful when faced with no training data. The deep learning training would benefit from retraining using the \gls{MSSSIM} metric of evaluation and if possible, clean images. It would be interesting to incorporate other experimental setup datasets, and try pretrained networks, or explore unsupervised approaches such as Deep Image Prior and Noise2Void.

The impact of this work could enable more efficient data acquisition and analysis. The methods developed here streamlining experiments at large-scale facilities such as \glspl{FEL} or table-top laboratory setups, optimizing beamtime usage and experimental parameter space that can be explored more efficiently and thus also expanded. In addition, the knowledge about photon counting statistics and denoising methods can potentially be applied to other experiments, such as X-ray diffraction, scattering, and other multidimensional spectroscopies. These methods are also of particular interest for applications outside the field involved with sparse, noisy datasets, such as medical imaging, astronomy, and particle physics.

Despite its successes, this work faced several limitations, including restricted test datasets and the lack of clean images for metric evaluation. In a future study, these limitations should be addressed, which could explore validation with domain-specific metrics, and expand datasets to a wider range of noise levels and experimental conditions.

While this work has addressed several critical challenges in \gls{MPES} data processing, future studies should make considerations for designing specialized experiments which directly study the statistical properties of photoemitted electrons at \gls{FEL} and \gls{HHG} sources. By carefully varying acquisition parameters, including photon energy, pulse duration, and repetition rate, the relationship between counting statistics and photoemission processes could be probed in greater depth. These experiments provide insights into how stochastic light source impacts the electronic structure of the material and hence could hint at quantum or correlation effects in photoemission. For instance, deviations from expected \gls{NB} statistics could reveal subtle many-body effects, electron correlation phenomena, or nonlinear interactions specific to the photoemission process.

The reported results could bridge the gap in experimental and theoretical efforts to understand the fundamental properties of matter, improving both denoising techniques and the understanding of intrinsic physics at work in MPES and related spectroscopies.
