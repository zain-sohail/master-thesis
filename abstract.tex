% % In the realm of photoemission spectroscopy, the exploration of large multi-dimensional phase spaces necessitates time-intensive data acquisition to ensure statistical robustness. Despite the unparalleled capabilities of free-electron lasers (FELs), in peak brightness and ultra- short pulsed X-rays, the limitations of low repetition rates prolong the data acquisition process. This impedes the agility of decision making that could otherwise enhance experimental results in the limited and valuable beamtime. By employing denoising strategies to mitigate noise while preserving intrinsic information, our proposed approach aims to streamline the data acquisition process, and effectively manage the escalating size and complexity of multi-dimensional photoemission data.
% We present a deep learning approach based on the Noise2Noise framework to denoise multidimensional photoemission spectroscopy (MPES) data obtained with a time-of-flight momentum microscope. Specifically, a 3D U-Net architecture is trained using low- and high-count noisy data, enabling the model to learn noise characteristics without requiring clean images. Our approach excels at reconstructing images even at extremely low count levels (order of 10^-3 counts/pixel), where conventional denoising techniques simply fail. Tests show that a 10-min acquisition processed with our deep learning model resolves major features not even visible after multiple hours of measurement. The presented approach has the potential to streamline the MPES data acquisition process at table-top/laboratory sources as well as large-scale facilities like FEL FLASH. By utilizing our method in future studies, researchers will be able to efficiently optimize acquisition parameters; thus, significant beamtime could be conserved, or an existing beamtime budget could be used more effectively, allowing for the exploration of a broader parameter space.

In the realm of photoemission spectroscopy, the exploration of large multi-dimensional phase spaces necessitates time-intensive data acquisition to ensure statistical robustness. Despite the unparalleled capabilities of free-electron lasers (FELs), in peak brightness and ultra- short pulsed X-rays, the limitations of low repetition rates prolong the data acquisition process. This impedes the agility of decision making that could otherwise enhance experimental results in the limited and valuable beamtime. By employing denoising strategies to mitigate noise while preserving intrinsic information, our proposed approach aims to streamline the data acquisition process, and effectively manage the escalating size and complexity of multi-dimensional photoemission data.

We present an investigation into advanced denoising methodologies for multidimensional photoemission spectroscopy (MPES) data acquired with time-of-flight momentum microscopes. Our study focuses on two key approaches: (1) classical denoising with BM3D combined with variance stabilization via the Anscombe transform and (2) a 3D deep learning approach, using the UNET architecture, based on the Noise2Noise paradigm. 

In addition, we analyze that photoemitted electrons acquired using SASE free-electron lasers (FEL) deviate from the commonly assumed Poissonian statistics and instead adhere to a negative binomial distribution, reflecting the overdispersion intrinsic to these systems. 

Despite the non-Poissonian statistics Anscombe-BM3D approach shows promising results in noisy images due to moderate counts (order of \num{1e-2} average counts/voxel). Whereas, our deep learning approach demonstrates exceptional performance, particularly in extreme low-count regimes (order of \num{1e-3} average counts/voxel), where conventional denoising techniques fail. Remarkably, we show that MPES datasets from a 10-minute acquisition processed with our trained model can reveal major features that are unobserved even after hours of measurement. 

These methodologies have hence the potential to streamline data acquisition at both laboratory-scale table-top setups and large-scale facilities like FEL FLASH. By optimizing acquisition parameters, researchers can conserve valuable beamtime or extend the scope of their studies to broader parameter spaces, results that hold broader implications for related experimental techniques.