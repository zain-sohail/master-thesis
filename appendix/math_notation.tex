\chapter{Mathematical Background}
\label{appendix:mathematical-notation}

\section{Law of Large Numbers}\label{section:law-of-large-numbers}
Consider a sequence of \gls{iid} random variables $X_1, X_2, \dots, X_n$ defined on the probability space $(\Omega, \mathcal{A}, P)$ (see \cref{section:probability-notation}), representing the number of detected events in different intervals. Since the random variable are \gls{iid}, the expected value $\mu$ of each $X_i$ is the same. The true mean (or expected value) of these random variables can be defined as $\mu = \mathbb{E}(X_1)$, and the variance as $\sigma^2 = Var(X_1)$.

Formally, by the law of large numbers\footnote{Specifically, the weak law of large numbers, which also has a stronger variant proving almost surely (a.s.) convergence.}, the sample mean $\bar{X_n} = \frac{1}{n} \sum_{i=1}^{n} X_i$ converges in probability to the expected value $\mu$ as $n \to \infty$ \cite{fellerIntroductionProbabilityTheory1991a}:
\begin{equation}
    \lim_{n \to \infty} P(|\bar{X_n} - \mu| > \epsilon) = 0
\end{equation}

The rate of convergence to $\bar{X_n}$ is of practical importance, as it indicates how quickly the sample statistics approximate the true distribution:
\begin{equation}
    \left|\bar{X_n} - \mu\right| = \mathcal{O}\left(\frac{1}{\sqrt{n}}\right)
\end{equation}

indicating that the relative fluctuations decrease with an increased number of observations \gls{ncounts}.  Increasing the acquisition time $T$ reduces the sample mean fluctuations, since $n \propto T$. While convergence rate described by $\mathcal{O}\left(\frac{1}{\sqrt{n}}\right)$ is a fundamental result, the constant that accompanies this rate is distribution dependent, and can affect the convergence rate. The Big O notation, denoted as $\mathcal{O}(g(n))$, describes an upper bound on the time complexity of an algorithm or the growth rate of a function.

\section{Measure Space and Measures}
\label{section:measurable_space}
\begin{note}
    {Measurable Space}

    Let $\Omega = \emptyset$, $P(\Omega)$ the power set of $\Omega$ and $\mathcal{A} \subset P(\Omega)$. If the following conditions are met, $\mathcal{A}$ is a called $\sigma$-algebra, and the $(\Omega, \mathcal{A})$ pair make up a measurable space.
    \begin{enumerate}
        \item $\Omega \in \mathcal{A}$.
        \item If $A \in \mathcal{A}$, then $\Omega \setminus A \in \mathcal{A}$.
        \item If $(A_n)_{n \in \mathbb{N}}$ is a sequence of sets where $A_n \in \mathcal{A}$ for all $n \in \mathbb{N}$, then 
        \begin{equation*}
            \bigcup_{n \in \mathbb{N}} A_n \in \mathcal{A}.
        \end{equation*}
    \end{enumerate}
\end{note}

\begin{note}
    {Positive Measure}
    Let $(\Omega, \mathcal{A})$ be a measurable space as defined in above. A function $\mu: \mathcal{A} \to [0, \infty]$ is called a \textit{positive measure} if it satisfies the following conditions:

    \begin{enumerate}
        \item $\mu(\emptyset) = 0$, (the measure of the empty set is zero)
        \item $\mu$ is \textit{countably additive}: For any countable collection of disjoint sets $(A_i)_{i \in \mathbb{N}} \subset \mathcal{A}$, we have
        \begin{equation*}
            \mu\left( \bigcup_{i=1}^{\infty} A_i \right) = \sum_{i=1}^{\infty} \mu(A_i).
        \end{equation*}
    \end{enumerate}
    
    If these conditions are met, then $\mu$ is called a \textit{measure} on the measurable space $(\Omega, \mathcal{A})$, and the triple $(\Omega, \mathcal{A}, \mu)$ is called a \textit{measure space}.
\end{note}



\section{Probability}\label{section:probability-notation}
The theory of probability is necessary to quantify stochastic and uncertain quantities. Throughout this text, it can be seen used the in context of inherently stochastic processes, such as quantum effects, and to quantify uncertainty in measurements.

\begin{note}
    {Probability Measure}
A \textit{probability measure} $P: \mathcal{A} \to [0, 1]$ satisfies all the properties of a positive measure (see Section \ref{section:measurable_space}), with the additional property known as the normalization condition:

\begin{equation*}
    P(\Omega) = 1
\end{equation*}

This leads to the measure space for probability (or probability space) being $(\Omega, \mathcal{A}, P)$. The probability of an event $A \in \mathcal{A}$ is $P(A)$. The probability measure is hence a real number between 0 and 1 that defines the likelihood of an event to occur.
\end{note}

% In the \textit{frequentist} interpretation, probability is defined as the long-run relative frequency of an event occurring in repeated independent trials. It assumes that probabilities are objective and intrinsic properties of the physical world. For example, the probability of getting heads in a fair coin toss is 0.5, meaning that if we were to toss the coin an infinite number of times, half of the outcomes would be heads.

% The \textit{Bayesian} interpretation, on the other hand, views probability as a measure of belief or certainty about an event, given the available information. It is inherently subjective and updates as new evidence is introduced. For instance, if we initially believe that a coin is fair, but after observing several tosses we notice a bias, we update our belief (and hence the probability) to reflect the new evidence. This process of updating beliefs is formalized through \textit{Bayes' rule}, which is a cornerstone of Bayesian inference.

% \begin{note}{Law of Total Probability}
%     The law of total probability provides a way to compute the probability of an event based on a partition of the sample space. If $\{B_i\}_{i=1}^{n}$ is a partition of the sample space $\Omega$ (i.e., $B_i \cap B_j = \emptyset$ for $i \neq j$ and $\bigcup_{i=1}^{n} B_i = \Omega$), then for any event $A$:

%     \begin{equation}
%         P(A) = \sum_{i=1}^{n} P(A \mid B_i) P(B_i).
%     \end{equation}
    
%     This law is particularly useful when dealing with complex events that can be decomposed into simpler, mutually exclusive cases.
    
% \end{note}

\subsection*{Distributions}
\begin{note}
    {Poisson distribution}\label{note:poisson-distribution}
    The \gls{PMF} for Poisson distribution Poi\((\lambda)\) with \(\lambda > 0\) is defined as
    \begin{equation}\label{eq:poisson-pmf}
        P(n;\lambda) = \frac{\lambda^n e^{-\lambda}}{n!}, \quad n \in \mathbb{N}_0
    \end{equation}
    \begin{enumerate}
        \item $\lambda = E(X) = Var(X)$ 
        \item Additivity: If $X_1 \sim \text{Poi}(\lambda_1)$ and $X_2 \sim \text{Poi}(\lambda_2)$ are independent Poisson random variables, then the sum $X_1 + X_2$ also follows a Poisson distribution with parameter $\lambda_1 + \lambda_2$
        \begin{equation}
            X_1 + X_2 \sim \text{Poi}(\lambda_1 + \lambda_2).
        \end{equation}
        
        This property is useful when considering counts from multiple independent sources.
    \end{enumerate}
\end{note}

For overdispersed count data, the \gls{NB} is suitable. 

\begin{note}
    {Negative Binomial Distribution}\label{note:negative-binomial-distribution}
    The \gls{PMF} for the \gls{NB} distribution, denoted $\text{NB}(r, p)$, where:
    \begin{itemize}
        \item $r > 0$ is the number of successes,
        \item $p \in (0, 1)$ is the probability of success on each trial,
    \end{itemize}
    is defined as:
    \begin{equation}\label{eq:nb-pmf}
        P(k; r, p) = \binom{k+r-1}{r-1} (1-p)^k p^r, \quad k \in \mathbb{N}_0
    \end{equation}
    where $k$ is the number of failures that occur before the $r$-th success.
    For $X \sim \text{NB}(r, p)$, the mean is given by:
        \begin{equation}
            E(X) = \frac{r(1-p)}{p}
        \end{equation}
        and the variance is:
        \begin{equation}
            Var(X) = \frac{r(1-p)}{p^2}
        \end{equation}

\end{note}


For relation between Poisson, \gls{NB} and Gamma distributions, see \cite{barry2020gamma}. It can be shown that \gls{NB} converges to Poisson as $r \to \infty$. The \gls{NB} distribution can also be considered as a mixture of Poisson distributions with a Gamma prior on the rate parameter $\lambda$.

% \subsection{Central Limit Theorem}
% % Central Limit Theorem (CLT)
% Consider again a sequence of \gls{iid} random variables $X_1, X_2, \dots, X_n$ defined on the probability space $(\Omega, \mathcal{A}, P)$. The Central Limit Theorem states that for such an \gls{iid} sequence with mean $\mu$ and variance $\sigma^2$, the normalized sum of these variables converges in distribution to the standard normal distribution as $n$ tends to infinity:

% \begin{equation}
%     \frac{\sum_{i=1}^{n} X_i - n\mu}{\sigma \sqrt{n}} \xrightarrow{d} \mathcal{N}(0, 1) \quad \text{as} \quad n \to \infty.
% \end{equation}

% \begin{note}{Conditional Probability}
%     Conditional probability quantifies the probability of an event occurring given that another event has already occurred. If $A$ and $B$ are two events in a probability space with $P(B) > 0$, the \textit{conditional probability} of $A$ given $B$ is defined as:
    
%     \begin{equation}
%         P(A \mid B) = \frac{P(A \cap B)}{P(B)}.
%     \end{equation}
%     This measure allows us to update the likelihood of an event based on new information provided by the occurrence of another event.
%     \end{note}
    
%     \begin{note}{Marginal Probability}
%     Marginal probability, also known as the \textit{unconditional probability}, refers to the probability of an event irrespective of the outcomes of other variables. If $A$ is an event in a probability space, the marginal probability of $A$ is simply $P(A)$. 
    
%     For discrete random variables $X$ and $Y$, the marginal probability of $X$ can be found by summing the joint probabilities over all values of $Y$:
    
%     \begin{equation}
%         P(X = x) = \sum_{y} P(X = x, Y = y).
%     \end{equation}
    
%     Similarly, for continuous random variables, the marginal probability density function is obtained by integrating the joint probability density function over all possible values of the other variables.
    
%     \end{note}

    
% Another interpretation:
% \begin{note}{Bayes' Rule}
% Bayes' rule is a fundamental theorem in probability theory that relates conditional and marginal probabilities. Given two events $A$ and $B$ with $P(B) > 0$, Bayes' rule states:

% \begin{equation}
%     P(A \mid B) = \frac{P(B \mid A) P(A)}{P(B)}.
% \end{equation}

% Bayes' rule allows us to update our beliefs about the probability of event $A$ occurring based on new evidence represented by event $B$. It forms the backbone of Bayesian inference, where prior beliefs are updated with new data.

% \end{note}

% \begin{note}{Application of Bayes' Rule}
%     Bayes' rule can be extended using the law of total probability:
    
%     \begin{equation}
%         P(A \mid B) = \frac{P(B \mid A) P(A)}{\sum_{i} P(B \mid A_i) P(A_i)},
%     \end{equation}
    
%     where $\{A_i\}$ is a partition of the sample space. This formula is useful in scenarios involving multiple hypotheses or models.
% \end{note}

% \section{Landau Notation}
% Landau notation, commonly referred to as Big O notation and its relatives (Big Omega, Big Theta, etc.), is used to describe the asymptotic behavior of functions. This is particularly important in the analysis of algorithms and in expressing the growth rates of functions.

% \section*{Big O Notation ($\mathcal{O}$)}
% Big O notation, denoted as $\mathcal{O}(g(n))$, describes an upper bound on the time complexity of an algorithm or the growth rate of a function. It provides an asymptotic upper bound on the function.

% % \begin{equation}
% % f(n) = O(g(n)) \quad \text{if and only if there exist positive constants } C \text{ and } n_0 \text{ such that for all } n \geq n_0, \; |f(n)| \leq C \cdot |g(n)|.
% % \end{equation}

% \section*{Big Omega Notation ($\Omega$)}
% Big Omega notation, denoted as $\Omega(g(n))$, describes a lower bound on the time complexity or growth rate of a function. It provides an asymptotic lower bound.

% % \begin{equation}
% % f(n) = \Omega(g(n)) \quad \text{if and only if there exist positive constants } C \text{ and } n_0 \text{ such that for all } n \geq n_0, \; |f(n)| \geq C \cdot |g(n)|.
% % \end{equation}

% \section*{Big Theta Notation}
% Big Theta notation, denoted as $\Theta(g(n))$, provides both an upper and lower bound on the growth rate of a function. It effectively means that $(f(n)$) grows asymptotically at the same rate as $(g(n)$).

% % \begin{equation}
% % f(n) = \Theta(g(n)) \quad \text{if and only if there exist positive constants } C_1, C_2 \text{ and } n_0 \text{ such that for all } n \geq n_0, \; C_1 \cdot |g(n)| \leq |f(n)| \leq C_2 \cdot |g(n)|.
% % \end{equation}

% \section*{Little o Notation}
% Little o notation, denoted as \(o(g(n))\), describes an upper bound that is not asymptotically tight. In other words, \(f(n)\) grows strictly slower than \(g(n)\).

% % \begin{equation}
% % f(n) = o(g(n)) \quad \text{if and only if \(\lim_{n \to \infty} \frac{f(n)}{g(n)} = 0\).
% % \end{equation}

% \section*{Little Omega Notation}
% Little omega notation, denoted as $\omega(g(n))$, describes a lower bound that is not asymptotically tight. This means that \(f(n)\) grows strictly faster than $(g(n)$).

% % \begin{equation}
% % f(n) = \omega(g(n)) \quad \text{if and only if \(\lim_{n \to \infty} \frac{f(n)}{g(n)} = \infty\).
% % \end{equation}

% \section*{Usage in Analysis}
% Landau notation is extensively used in algorithm analysis to describe the running time or space requirements of an algorithm as a function of the input size. It abstracts away constants and less significant terms, focusing on the dominant factor that impacts growth rate as input size increases.


\section{Statistical Inference}
% For goodness-of-fit tests, small $p$-values indicate that you can reject the null hypothesis and conclude that your data were not drawn from a population with the specified distribution. In the case of such tests, a high $p$-value would suggest that the observed data and the expected distribution are statistically similar.
% \subsection*{Goodness of Fit Tests}\label{section:chi-test}
% We hypothesize that the data follows a certain distribution based on theoretical grounding and visualize analysis. Specifically, that data from a laser follows a Poisson distribution and from an \gls{FEL} a \gls{NB} distribution. One possible way to test these hypotheses is through a statistical test such as the Chi-Square Goodness of Fit test.

% The Chi-Square test determines if the observed frequencies of data fit with the expected frequencies derived from the hypothesized distribution. This test is particularly suitable for discrete data, making it appropriate for count data that follows Poisson or Negative Binomial distributions.

% \begin{note}
%     {Chi-Square Test}
%     Let  $O_i$  be the observed frequency in category  $i$ , and  $E_i$  be the expected frequency in category  $i$  under the hypothesized distribution. The Chi-Square test statistic  $\chi^2 $ is given by:
%     \begin{equation}
%         \chi^2 = \sum_{i=1}^k \frac{(O_i - E_i)^2}{E_i}
%     \end{equation}
%     where $k$ is the number of categories. The degrees of freedom for this test is given by:

%     The null hypothesis $H_0$ states that the observed data follows the specified distribution, while the alternative hypothesis $H_1$ posits that the observed data does not follow the specified distribution:

%     \begin{equation*}
%         H_0: \text{The data follows the specified distribution}
%     \end{equation*}
%     \begin{equation*}
%         H_1: \text{The data does not follow the specified distribution}
%     \end{equation*}
% \end{note}

% The Poisson dispersion test specifically examines whether the variance of the observed data significantly differs from the mean, as expected for a Poisson distribution, where the mean and variance should be equal. Deviations from this relationship may indicate overdispersion or underdispersion, which can inform the choice of an appropriate statistical model.
% \begin{note}
%     {Poisson Dispersion Test}
%     Let $O_i$ be the observed frequency in category $i$ and let $\lambda$ be the estimated mean of the observed data. The test statistic for the Poisson dispersion test is defined as:
%     \begin{equation}
%     D = \frac{S^2}{\bar{X}}
%     \end{equation}
%     where $S^2$ is the sample variance of the observed data and $\bar{X}$ is the mean of the observed counts.
%     Under the null hypothesis $H_0$ (the data follows a Poisson distribution), the test statistic $D$ follows a Chi-Square distribution with $n - 1$ degrees of freedom, where $n$ is the number of observed categories. 

% The null hypothesis and alternative hypothesis are stated as follows:

% \begin{equation*}
%     H_0: \text{The data follows a Poisson distribution}
% \end{equation*}
% \begin{equation*}
%     H_1: \text{The data does not follow a Poisson distribution}
% \end{equation*}
% \end{note}


\subsection*{Confidence Intervals}
Confidence Intervals are a way to quantify the uncertainty in an estimate. They provide a range of plausible values for a parameter, rather than a single point estimate. The confidence level indicates the probability that the interval contains the true parameter value. For example, a 95\% confidence interval means that if we were to repeat the experiment many times, 95\% of the intervals would contain the true parameter value.

Parametric methods for computing confidence intervals rely on knowing the underlying distribution of the data, such as the normal distribution. However, in many cases, the true distribution is unknown or does not follow a standard form. In such cases, non-parametric methods such as bootstrapping can be used.

Bootstrapping involves resampling the data with replacement to create multiple bootstrap samples, from which the sample statistic is computed. The distribution of the sample statistic across the bootstrap samples provides an estimate of the sampling distribution of the statistic, and becomes more informative when using larger sample sizes.

% \subsection*{Optimal parameter estimation}
% Used in estimating distribution parameters